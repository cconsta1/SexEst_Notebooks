{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "\n",
    "# system\n",
    "\n",
    "import os\n",
    "\n",
    "# data analysis and plotting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import shapiro\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from xgboost import plot_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# data processing and model validation\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, explained_variance_score, confusion_matrix, accuracy_score, classification_report, log_loss\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "# classification libraries\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel, Matern, RationalQuadratic\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Importing imputation libs. \n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Hyperparameter optimization\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Missing data models\n",
    "\n",
    "from itertools import combinations\n",
    "#from sklearn.linear_model import LogisticRegressionCV\n",
    "#from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# web stuff\n",
    "import pickle\n",
    "\n",
    "# Various parameter settings\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# To install sklearn type \"pip install numpy scipy scikit-learn\" to the anaconda terminal\n",
    "\n",
    "# To change scientific numbers to float\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "\n",
    "# Increases the size of sns plots\n",
    "sns.set(rc={'figure.figsize':(12,10)})\n",
    "\n",
    "# import sys\n",
    "# !conda list Check the packages installed\n",
    "\n",
    "# Displaying all the rows/columns in a data set (the default option is not to show them)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data and create the datasets needed in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the raw data\n",
    "\n",
    "raw_data_goldman = pd.read_csv(\"datasets/Goldman.csv\", header = 0, encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Element:</th>\n",
       "      <th>LHUM</th>\n",
       "      <th>RHUM</th>\n",
       "      <th>LRAD</th>\n",
       "      <th>RRAD</th>\n",
       "      <th>LFEM</th>\n",
       "      <th>RFEM</th>\n",
       "      <th>LTIB</th>\n",
       "      <th>RTIB</th>\n",
       "      <th>OSCX</th>\n",
       "      <th>Metrics:</th>\n",
       "      <th>LHML</th>\n",
       "      <th>LHEB</th>\n",
       "      <th>LHHD</th>\n",
       "      <th>LHMLD</th>\n",
       "      <th>LHAPD</th>\n",
       "      <th>RHML</th>\n",
       "      <th>RHEB</th>\n",
       "      <th>RHHD</th>\n",
       "      <th>RHMLD</th>\n",
       "      <th>RHAPD</th>\n",
       "      <th>LRML</th>\n",
       "      <th>LRMLD</th>\n",
       "      <th>LRAPD</th>\n",
       "      <th>RRML</th>\n",
       "      <th>RRMLD</th>\n",
       "      <th>RRAPD</th>\n",
       "      <th>LFML</th>\n",
       "      <th>LFBL</th>\n",
       "      <th>LFEB</th>\n",
       "      <th>LFAB</th>\n",
       "      <th>LFHD</th>\n",
       "      <th>LFMLD</th>\n",
       "      <th>LFAPD</th>\n",
       "      <th>RFML</th>\n",
       "      <th>RFBL</th>\n",
       "      <th>RFEB</th>\n",
       "      <th>RFAB</th>\n",
       "      <th>RFHD</th>\n",
       "      <th>RFMLD</th>\n",
       "      <th>RFAPD</th>\n",
       "      <th>LTML</th>\n",
       "      <th>LTPB</th>\n",
       "      <th>LTMLD</th>\n",
       "      <th>LTAPD</th>\n",
       "      <th>RTML</th>\n",
       "      <th>RTPB</th>\n",
       "      <th>RTMLD</th>\n",
       "      <th>RTAPD</th>\n",
       "      <th>BIB</th>\n",
       "      <th>LIBL</th>\n",
       "      <th>RIBL</th>\n",
       "      <th>LAcH</th>\n",
       "      <th>RAcH</th>\n",
       "      <th>Derived:</th>\n",
       "      <th>Brachial</th>\n",
       "      <th>Crural</th>\n",
       "      <th>IL UL/LL</th>\n",
       "      <th>IL LL/UL</th>\n",
       "      <th>CBR FHD</th>\n",
       "      <th>McH FHD</th>\n",
       "      <th>GRINE FHD</th>\n",
       "      <th>AVG FHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1376.000000</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>1368.000000</td>\n",
       "      <td>1376.000000</td>\n",
       "      <td>1376.000000</td>\n",
       "      <td>1403.000000</td>\n",
       "      <td>1384.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1403.000000</td>\n",
       "      <td>1402.000000</td>\n",
       "      <td>1321.000000</td>\n",
       "      <td>1321.000000</td>\n",
       "      <td>1321.000000</td>\n",
       "      <td>1337.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1421.000000</td>\n",
       "      <td>1416.000000</td>\n",
       "      <td>1380.000000</td>\n",
       "      <td>1378.000000</td>\n",
       "      <td>1421.000000</td>\n",
       "      <td>1423.000000</td>\n",
       "      <td>1423.000000</td>\n",
       "      <td>1426.000000</td>\n",
       "      <td>1423.000000</td>\n",
       "      <td>1386.000000</td>\n",
       "      <td>1390.000000</td>\n",
       "      <td>1435.000000</td>\n",
       "      <td>1424.000000</td>\n",
       "      <td>1424.000000</td>\n",
       "      <td>1403.000000</td>\n",
       "      <td>1352.000000</td>\n",
       "      <td>1399.000000</td>\n",
       "      <td>1398.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1349.000000</td>\n",
       "      <td>1395.000000</td>\n",
       "      <td>1394.000000</td>\n",
       "      <td>1469.000000</td>\n",
       "      <td>1179.000000</td>\n",
       "      <td>1177.000000</td>\n",
       "      <td>1371.000000</td>\n",
       "      <td>1375.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1463.000000</td>\n",
       "      <td>1490.000000</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>1519.000000</td>\n",
       "      <td>1519.000000</td>\n",
       "      <td>1519.000000</td>\n",
       "      <td>1519.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100780</td>\n",
       "      <td>0.081274</td>\n",
       "      <td>0.140442</td>\n",
       "      <td>0.126788</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>0.064369</td>\n",
       "      <td>0.086476</td>\n",
       "      <td>0.086476</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>303.759811</td>\n",
       "      <td>57.442770</td>\n",
       "      <td>42.741615</td>\n",
       "      <td>19.884404</td>\n",
       "      <td>19.717754</td>\n",
       "      <td>307.556379</td>\n",
       "      <td>58.194581</td>\n",
       "      <td>43.003059</td>\n",
       "      <td>20.362117</td>\n",
       "      <td>20.462511</td>\n",
       "      <td>233.068887</td>\n",
       "      <td>14.097017</td>\n",
       "      <td>11.188312</td>\n",
       "      <td>234.964099</td>\n",
       "      <td>14.493866</td>\n",
       "      <td>11.335993</td>\n",
       "      <td>427.106967</td>\n",
       "      <td>423.455508</td>\n",
       "      <td>76.067754</td>\n",
       "      <td>66.361168</td>\n",
       "      <td>43.430837</td>\n",
       "      <td>25.767442</td>\n",
       "      <td>27.171792</td>\n",
       "      <td>425.657433</td>\n",
       "      <td>421.620169</td>\n",
       "      <td>76.259019</td>\n",
       "      <td>66.280950</td>\n",
       "      <td>43.497401</td>\n",
       "      <td>25.440857</td>\n",
       "      <td>27.243588</td>\n",
       "      <td>353.078403</td>\n",
       "      <td>69.344305</td>\n",
       "      <td>21.457984</td>\n",
       "      <td>26.351710</td>\n",
       "      <td>352.419429</td>\n",
       "      <td>69.343217</td>\n",
       "      <td>21.985778</td>\n",
       "      <td>25.372317</td>\n",
       "      <td>262.395848</td>\n",
       "      <td>150.980068</td>\n",
       "      <td>151.104503</td>\n",
       "      <td>48.873508</td>\n",
       "      <td>48.923360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765279</td>\n",
       "      <td>0.827734</td>\n",
       "      <td>0.692225</td>\n",
       "      <td>1.445639</td>\n",
       "      <td>60.145433</td>\n",
       "      <td>57.412799</td>\n",
       "      <td>62.073215</td>\n",
       "      <td>59.877149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301135</td>\n",
       "      <td>0.273345</td>\n",
       "      <td>0.347558</td>\n",
       "      <td>0.332844</td>\n",
       "      <td>0.255602</td>\n",
       "      <td>0.245489</td>\n",
       "      <td>0.281157</td>\n",
       "      <td>0.281157</td>\n",
       "      <td>0.140580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.025881</td>\n",
       "      <td>5.446128</td>\n",
       "      <td>4.134822</td>\n",
       "      <td>2.472297</td>\n",
       "      <td>2.345870</td>\n",
       "      <td>23.116218</td>\n",
       "      <td>5.544286</td>\n",
       "      <td>4.257656</td>\n",
       "      <td>2.573130</td>\n",
       "      <td>2.381074</td>\n",
       "      <td>18.985815</td>\n",
       "      <td>1.898293</td>\n",
       "      <td>1.321821</td>\n",
       "      <td>18.904538</td>\n",
       "      <td>1.938423</td>\n",
       "      <td>1.364567</td>\n",
       "      <td>31.509788</td>\n",
       "      <td>31.597694</td>\n",
       "      <td>6.204072</td>\n",
       "      <td>5.857932</td>\n",
       "      <td>3.972412</td>\n",
       "      <td>2.745311</td>\n",
       "      <td>2.874594</td>\n",
       "      <td>31.751905</td>\n",
       "      <td>31.843808</td>\n",
       "      <td>6.238232</td>\n",
       "      <td>5.832092</td>\n",
       "      <td>4.053790</td>\n",
       "      <td>2.663129</td>\n",
       "      <td>2.915361</td>\n",
       "      <td>28.076348</td>\n",
       "      <td>5.773576</td>\n",
       "      <td>2.376611</td>\n",
       "      <td>3.022335</td>\n",
       "      <td>28.749863</td>\n",
       "      <td>5.775864</td>\n",
       "      <td>2.468692</td>\n",
       "      <td>2.783156</td>\n",
       "      <td>18.274559</td>\n",
       "      <td>10.573547</td>\n",
       "      <td>10.753604</td>\n",
       "      <td>4.052208</td>\n",
       "      <td>4.079195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>0.027455</td>\n",
       "      <td>0.018438</td>\n",
       "      <td>0.038447</td>\n",
       "      <td>7.946894</td>\n",
       "      <td>8.914306</td>\n",
       "      <td>9.029766</td>\n",
       "      <td>8.555195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.500000</td>\n",
       "      <td>36.510000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>12.010000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>32.240000</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>13.840000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>8.740000</td>\n",
       "      <td>7.370000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>9.160000</td>\n",
       "      <td>7.880000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>309.500000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>49.990000</td>\n",
       "      <td>33.640000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>18.880000</td>\n",
       "      <td>341.500000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>49.830000</td>\n",
       "      <td>32.970000</td>\n",
       "      <td>17.970000</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>15.220000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>18.590000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>38.640000</td>\n",
       "      <td>37.890000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677912</td>\n",
       "      <td>0.692025</td>\n",
       "      <td>0.599260</td>\n",
       "      <td>1.299290</td>\n",
       "      <td>36.092754</td>\n",
       "      <td>34.692285</td>\n",
       "      <td>39.058420</td>\n",
       "      <td>38.300225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>287.375000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>39.760000</td>\n",
       "      <td>18.230000</td>\n",
       "      <td>18.030000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>39.900000</td>\n",
       "      <td>18.690000</td>\n",
       "      <td>18.732500</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>12.780000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>13.117500</td>\n",
       "      <td>10.327500</td>\n",
       "      <td>404.500000</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>62.010000</td>\n",
       "      <td>40.610000</td>\n",
       "      <td>23.830000</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>61.970000</td>\n",
       "      <td>40.510000</td>\n",
       "      <td>23.535000</td>\n",
       "      <td>25.097500</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>19.780000</td>\n",
       "      <td>24.180000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>20.245000</td>\n",
       "      <td>23.380000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>45.995000</td>\n",
       "      <td>46.055000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.744615</td>\n",
       "      <td>0.809212</td>\n",
       "      <td>0.680304</td>\n",
       "      <td>1.421166</td>\n",
       "      <td>54.614902</td>\n",
       "      <td>50.992205</td>\n",
       "      <td>55.569460</td>\n",
       "      <td>53.738995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>303.500000</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>42.715000</td>\n",
       "      <td>19.855000</td>\n",
       "      <td>19.640000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>43.015000</td>\n",
       "      <td>20.300000</td>\n",
       "      <td>20.460000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>14.050000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>11.270000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>66.615000</td>\n",
       "      <td>43.540000</td>\n",
       "      <td>25.730000</td>\n",
       "      <td>27.160000</td>\n",
       "      <td>426.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>66.190000</td>\n",
       "      <td>43.520000</td>\n",
       "      <td>25.370000</td>\n",
       "      <td>27.135000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>21.370000</td>\n",
       "      <td>26.235000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>21.890000</td>\n",
       "      <td>25.215000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>49.050000</td>\n",
       "      <td>49.030000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765331</td>\n",
       "      <td>0.828423</td>\n",
       "      <td>0.691577</td>\n",
       "      <td>1.445971</td>\n",
       "      <td>60.046353</td>\n",
       "      <td>57.474110</td>\n",
       "      <td>62.135320</td>\n",
       "      <td>59.832163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>45.722500</td>\n",
       "      <td>21.460000</td>\n",
       "      <td>21.300000</td>\n",
       "      <td>323.500000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>22.035000</td>\n",
       "      <td>22.187500</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>15.320000</td>\n",
       "      <td>12.120000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>15.742500</td>\n",
       "      <td>12.312500</td>\n",
       "      <td>448.500000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>70.822500</td>\n",
       "      <td>46.150000</td>\n",
       "      <td>27.650000</td>\n",
       "      <td>29.175000</td>\n",
       "      <td>447.500000</td>\n",
       "      <td>443.500000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>70.605000</td>\n",
       "      <td>46.395000</td>\n",
       "      <td>27.242500</td>\n",
       "      <td>29.285000</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>23.030000</td>\n",
       "      <td>28.537500</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>23.790000</td>\n",
       "      <td>27.317500</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>51.715000</td>\n",
       "      <td>51.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.785749</td>\n",
       "      <td>0.845936</td>\n",
       "      <td>0.703647</td>\n",
       "      <td>1.469932</td>\n",
       "      <td>65.196007</td>\n",
       "      <td>63.737713</td>\n",
       "      <td>68.480050</td>\n",
       "      <td>65.676422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>56.330000</td>\n",
       "      <td>27.220000</td>\n",
       "      <td>27.290000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>55.670000</td>\n",
       "      <td>30.440000</td>\n",
       "      <td>27.190000</td>\n",
       "      <td>290.500000</td>\n",
       "      <td>22.150000</td>\n",
       "      <td>16.280000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>23.220000</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>531.000000</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>93.500000</td>\n",
       "      <td>83.680000</td>\n",
       "      <td>57.390000</td>\n",
       "      <td>34.540000</td>\n",
       "      <td>37.060000</td>\n",
       "      <td>532.500000</td>\n",
       "      <td>531.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>82.850000</td>\n",
       "      <td>57.860000</td>\n",
       "      <td>33.840000</td>\n",
       "      <td>37.530000</td>\n",
       "      <td>446.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>37.940000</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>29.830000</td>\n",
       "      <td>35.200000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>62.940000</td>\n",
       "      <td>62.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>0.965422</td>\n",
       "      <td>0.769651</td>\n",
       "      <td>1.668724</td>\n",
       "      <td>92.745113</td>\n",
       "      <td>89.122375</td>\n",
       "      <td>94.193500</td>\n",
       "      <td>92.020329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Element:         LHUM         RHUM         LRAD         RRAD  \\\n",
       "count       0.0  1538.000000  1538.000000  1538.000000  1538.000000   \n",
       "mean        NaN     0.100780     0.081274     0.140442     0.126788   \n",
       "std         NaN     0.301135     0.273345     0.347558     0.332844   \n",
       "min         NaN     0.000000     0.000000     0.000000     0.000000   \n",
       "25%         NaN     0.000000     0.000000     0.000000     0.000000   \n",
       "50%         NaN     0.000000     0.000000     0.000000     0.000000   \n",
       "75%         NaN     0.000000     0.000000     0.000000     0.000000   \n",
       "max         NaN     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              LFEM         RFEM         LTIB         RTIB         OSCX  \\\n",
       "count  1538.000000  1538.000000  1538.000000  1538.000000  1538.000000   \n",
       "mean      0.070221     0.064369     0.086476     0.086476     0.020156   \n",
       "std       0.255602     0.245489     0.281157     0.281157     0.140580   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       Metrics:         LHML         LHEB         LHHD        LHMLD  \\\n",
       "count       0.0  1376.000000  1354.000000  1368.000000  1376.000000   \n",
       "mean        NaN   303.759811    57.442770    42.741615    19.884404   \n",
       "std         NaN    23.025881     5.446128     4.134822     2.472297   \n",
       "min         NaN   229.500000    36.510000    29.580000    12.010000   \n",
       "25%         NaN   287.375000    53.000000    39.760000    18.230000   \n",
       "50%         NaN   303.500000    57.500000    42.715000    19.855000   \n",
       "75%         NaN   319.000000    61.000000    45.722500    21.460000   \n",
       "max         NaN   376.000000    75.000000    56.330000    27.220000   \n",
       "\n",
       "             LHAPD         RHML         RHEB         RHHD        RHMLD  \\\n",
       "count  1376.000000  1403.000000  1384.000000  1396.000000  1403.000000   \n",
       "mean     19.717754   307.556379    58.194581    43.003059    20.362117   \n",
       "std       2.345870    23.116218     5.544286     4.257656     2.573130   \n",
       "min      13.000000   234.000000    42.000000    32.240000    12.250000   \n",
       "25%      18.030000   291.000000    54.000000    39.900000    18.690000   \n",
       "50%      19.640000   307.000000    58.000000    43.015000    20.300000   \n",
       "75%      21.300000   323.500000    62.000000    46.000000    22.035000   \n",
       "max      27.290000   383.000000    75.000000    55.670000    30.440000   \n",
       "\n",
       "             RHAPD         LRML        LRMLD        LRAPD         RRML  \\\n",
       "count  1402.000000  1321.000000  1321.000000  1321.000000  1337.000000   \n",
       "mean     20.462511   233.068887    14.097017    11.188312   234.964099   \n",
       "std       2.381074    18.985815     1.898293     1.321821    18.904538   \n",
       "min      13.840000   179.000000     8.740000     7.370000   180.000000   \n",
       "25%      18.732500   219.000000    12.780000    10.230000   221.000000   \n",
       "50%      20.460000   233.000000    14.050000    11.110000   235.000000   \n",
       "75%      22.187500   247.000000    15.320000    12.120000   248.000000   \n",
       "max      27.190000   290.500000    22.150000    16.280000   291.000000   \n",
       "\n",
       "             RRMLD        RRAPD         LFML         LFBL         LFEB  \\\n",
       "count  1340.000000  1340.000000  1421.000000  1416.000000  1380.000000   \n",
       "mean     14.493866    11.335993   427.106967   423.455508    76.067754   \n",
       "std       1.938423     1.364567    31.509788    31.597694     6.204072   \n",
       "min       9.160000     7.880000   345.000000   309.500000    58.000000   \n",
       "25%      13.117500    10.327500   404.500000   401.000000    72.000000   \n",
       "50%      14.400000    11.270000   428.000000   424.000000    76.000000   \n",
       "75%      15.742500    12.312500   448.500000   445.000000    81.000000   \n",
       "max      23.220000    15.680000   531.000000   530.000000    93.500000   \n",
       "\n",
       "              LFAB         LFHD        LFMLD        LFAPD         RFML  \\\n",
       "count  1378.000000  1421.000000  1423.000000  1423.000000  1426.000000   \n",
       "mean     66.361168    43.430837    25.767442    27.171792   425.657433   \n",
       "std       5.857932     3.972412     2.745311     2.874594    31.751905   \n",
       "min      49.990000    33.640000    17.900000    18.880000   341.500000   \n",
       "25%      62.010000    40.610000    23.830000    25.120000   403.000000   \n",
       "50%      66.615000    43.540000    25.730000    27.160000   426.000000   \n",
       "75%      70.822500    46.150000    27.650000    29.175000   447.500000   \n",
       "max      83.680000    57.390000    34.540000    37.060000   532.500000   \n",
       "\n",
       "              RFBL         RFEB         RFAB         RFHD        RFMLD  \\\n",
       "count  1423.000000  1386.000000  1390.000000  1435.000000  1424.000000   \n",
       "mean    421.620169    76.259019    66.280950    43.497401    25.440857   \n",
       "std      31.843808     6.238232     5.832092     4.053790     2.663129   \n",
       "min     279.000000    58.000000    49.830000    32.970000    17.970000   \n",
       "25%     399.000000    72.000000    61.970000    40.510000    23.535000   \n",
       "50%     422.000000    76.000000    66.190000    43.520000    25.370000   \n",
       "75%     443.500000    81.000000    70.605000    46.395000    27.242500   \n",
       "max     531.000000    94.000000    82.850000    57.860000    33.840000   \n",
       "\n",
       "             RFAPD         LTML         LTPB        LTMLD        LTAPD  \\\n",
       "count  1424.000000  1403.000000  1352.000000  1399.000000  1398.000000   \n",
       "mean     27.243588   353.078403    69.344305    21.457984    26.351710   \n",
       "std       2.915361    28.076348     5.773576     2.376611     3.022335   \n",
       "min      18.340000   276.000000    52.000000    15.220000    19.000000   \n",
       "25%      25.097500   333.000000    65.000000    19.780000    24.180000   \n",
       "50%      27.135000   353.000000    69.500000    21.370000    26.235000   \n",
       "75%      29.285000   372.000000    74.000000    23.030000    28.537500   \n",
       "max      37.530000   446.000000    86.000000    31.000000    37.940000   \n",
       "\n",
       "              RTML         RTPB        RTMLD        RTAPD          BIB  \\\n",
       "count  1400.000000  1349.000000  1395.000000  1394.000000  1469.000000   \n",
       "mean    352.419429    69.343217    21.985778    25.372317   262.395848   \n",
       "std      28.749863     5.775864     2.468692     2.783156    18.274559   \n",
       "min     237.000000    50.000000    15.150000    18.590000   184.000000   \n",
       "25%     332.000000    65.000000    20.245000    23.380000   251.000000   \n",
       "50%     353.000000    69.000000    21.890000    25.215000   263.000000   \n",
       "75%     372.000000    74.000000    23.790000    27.317500   274.000000   \n",
       "max     444.000000    85.000000    29.830000    35.200000   324.000000   \n",
       "\n",
       "              LIBL         RIBL         LAcH         RAcH  Derived:  \\\n",
       "count  1179.000000  1177.000000  1371.000000  1375.000000       0.0   \n",
       "mean    150.980068   151.104503    48.873508    48.923360       NaN   \n",
       "std      10.573547    10.753604     4.052208     4.079195       NaN   \n",
       "min     105.000000   106.000000    38.640000    37.890000       NaN   \n",
       "25%     145.000000   145.000000    45.995000    46.055000       NaN   \n",
       "50%     151.000000   151.000000    49.050000    49.030000       NaN   \n",
       "75%     158.000000   158.000000    51.715000    51.800000       NaN   \n",
       "max     181.000000   189.000000    62.940000    62.990000       NaN   \n",
       "\n",
       "          Brachial       Crural     IL UL/LL     IL LL/UL      CBR FHD  \\\n",
       "count  1463.000000  1490.000000  1418.000000  1418.000000  1519.000000   \n",
       "mean      0.765279     0.827734     0.692225     1.445639    60.145433   \n",
       "std       0.031805     0.027455     0.018438     0.038447     7.946894   \n",
       "min       0.677912     0.692025     0.599260     1.299290    36.092754   \n",
       "25%       0.744615     0.809212     0.680304     1.421166    54.614902   \n",
       "50%       0.765331     0.828423     0.691577     1.445971    60.046353   \n",
       "75%       0.785749     0.845936     0.703647     1.469932    65.196007   \n",
       "max       1.076923     0.965422     0.769651     1.668724    92.745113   \n",
       "\n",
       "           McH FHD    GRINE FHD      AVG FHD  \n",
       "count  1519.000000  1519.000000  1519.000000  \n",
       "mean     57.412799    62.073215    59.877149  \n",
       "std       8.914306     9.029766     8.555195  \n",
       "min      34.692285    39.058420    38.300225  \n",
       "25%      50.992205    55.569460    53.738995  \n",
       "50%      57.474110    62.135320    59.832163  \n",
       "75%      63.737713    68.480050    65.676422  \n",
       "max      89.122375    94.193500    92.020329  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new data set that contains all the measured data, i.e.,\n",
    "# not derived data as well as the class label Sex for each measurement\n",
    "\n",
    "measured_data_goldman = raw_data_goldman.loc[:,\"LHML\":\"RAcH\"]\n",
    "\n",
    "# Fill missing data with zeroes so that we can average between left and right skeletal measurements below\n",
    "\n",
    "measured_data_goldman = measured_data_goldman.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               BIB          HML          HHD          RML          FML  \\\n",
      "count  1538.000000  1538.000000  1538.000000  1538.000000  1538.000000   \n",
      "mean    250.623862   298.197042    41.699971   225.650033   420.892230   \n",
      "std      57.194643    52.024020     8.025574    46.894808    57.592097   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%     249.000000   288.312500    39.520000   218.500000   402.750000   \n",
      "50%     262.000000   304.500000    42.700000   233.000000   426.375000   \n",
      "75%     274.000000   320.750000    45.728750   247.000000   447.937500   \n",
      "max     324.000000   379.500000    55.120000   290.250000   531.750000   \n",
      "\n",
      "               FBL          FHD          TML          FEB          TPB  \\\n",
      "count  1538.000000  1538.000000  1538.000000  1538.000000  1538.000000   \n",
      "mean    416.541125    42.925692   343.531632    74.052666    66.309655   \n",
      "std      59.136325     6.222401    63.412299    13.827096    15.227271   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%     399.000000    40.446250   330.812500    71.250000    64.500000   \n",
      "50%     422.500000    43.432500   352.500000    76.000000    69.000000   \n",
      "75%     444.250000    46.258750   372.000000    80.500000    73.500000   \n",
      "max     530.500000    57.625000   444.500000    93.250000    85.000000   \n",
      "\n",
      "               HEB  \n",
      "count  1538.000000  \n",
      "mean     56.191096  \n",
      "std      10.835338  \n",
      "min       0.000000  \n",
      "25%      53.500000  \n",
      "50%      57.500000  \n",
      "75%      61.500000  \n",
      "max      74.000000  \n"
     ]
    }
   ],
   "source": [
    "# Create new columns and take the average between left and right skeletal measurements\n",
    "\n",
    "target_cols = ['HML', 'HHD', 'RML', 'FML', 'FBL','FHD', 'TML', 'FEB', 'TPB', 'HEB']\n",
    "\n",
    "\n",
    "for col in target_cols:\n",
    "    measured_data_goldman[col] = 0.\n",
    "    \n",
    "    min_left_col_value = measured_data_goldman[\"\".join([\"L\",col])][measured_data_goldman[\"\".join([\"L\",col])] > 0.1].min() - 1\n",
    "    \n",
    "    min_right_col_value = measured_data_goldman[\"\".join([\"R\",col])][measured_data_goldman[\"\".join([\"R\",col])] > 0.1].min() - 1\n",
    "    \n",
    "    measured_data_goldman.loc[(measured_data_goldman[\"\".join([\"L\",col])] < 0.1) & (measured_data_goldman[\"\".join([\"R\",col])] > min_right_col_value), \n",
    "        col] = measured_data_goldman[\"\".join([\"R\",col])]\n",
    "\n",
    "    measured_data_goldman.loc[(measured_data_goldman[\"\".join([\"R\",col])] < 0.1) & (measured_data_goldman[\"\".join([\"L\",col])] > min_left_col_value), \n",
    "       col] = measured_data_goldman[\"\".join([\"L\",col])]\n",
    "\n",
    "    measured_data_goldman.loc[(measured_data_goldman[\"\".join([\"R\",col])] > min_right_col_value) & (measured_data_goldman[\"\".join([\"L\",col])] > min_left_col_value), \n",
    "       col] = (measured_data_goldman[\"\".join([\"L\",col])] + measured_data_goldman[\"\".join([\"R\",col])])/2\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create a dataset with the features we will use to build our models\n",
    "\n",
    "model_cols = ['BIB','HML', 'HHD', 'RML', 'FML', 'FBL','FHD', 'TML', 'FEB', 'TPB', 'HEB']\n",
    "\n",
    "model_data_goldman = measured_data_goldman.drop(columns=[col for col in measured_data_goldman if col not in model_cols])\n",
    "\n",
    "# Add the Sex column\n",
    "\n",
    "model_data_goldman = pd.concat([model_data_goldman.loc[:,:],raw_data_goldman.loc[:,\"Sex\"]],axis=1)\n",
    "\n",
    "print(model_data_goldman.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>HML</th>\n",
       "      <th>HHD</th>\n",
       "      <th>RML</th>\n",
       "      <th>FML</th>\n",
       "      <th>FBL</th>\n",
       "      <th>FHD</th>\n",
       "      <th>TML</th>\n",
       "      <th>FEB</th>\n",
       "      <th>TPB</th>\n",
       "      <th>HEB</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>250.544830</td>\n",
       "      <td>298.170681</td>\n",
       "      <td>41.714529</td>\n",
       "      <td>225.744928</td>\n",
       "      <td>420.889234</td>\n",
       "      <td>416.533704</td>\n",
       "      <td>42.939784</td>\n",
       "      <td>343.450524</td>\n",
       "      <td>74.060209</td>\n",
       "      <td>66.297120</td>\n",
       "      <td>56.201345</td>\n",
       "      <td>0.355366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57.361899</td>\n",
       "      <td>52.176096</td>\n",
       "      <td>8.047175</td>\n",
       "      <td>46.673734</td>\n",
       "      <td>57.745325</td>\n",
       "      <td>59.293408</td>\n",
       "      <td>6.237500</td>\n",
       "      <td>63.587332</td>\n",
       "      <td>13.867850</td>\n",
       "      <td>15.272226</td>\n",
       "      <td>10.868670</td>\n",
       "      <td>0.478781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>249.000000</td>\n",
       "      <td>288.250000</td>\n",
       "      <td>39.530000</td>\n",
       "      <td>218.500000</td>\n",
       "      <td>402.562500</td>\n",
       "      <td>398.937500</td>\n",
       "      <td>40.470000</td>\n",
       "      <td>330.687500</td>\n",
       "      <td>71.250000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>262.000000</td>\n",
       "      <td>304.500000</td>\n",
       "      <td>42.735000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>426.500000</td>\n",
       "      <td>422.625000</td>\n",
       "      <td>43.460000</td>\n",
       "      <td>352.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>320.750000</td>\n",
       "      <td>45.762500</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>447.812500</td>\n",
       "      <td>444.250000</td>\n",
       "      <td>46.277500</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>61.562500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>55.120000</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>531.750000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>57.625000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>93.250000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BIB          HML          HHD          RML          FML  \\\n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  1528.000000   \n",
       "mean    250.544830   298.170681    41.714529   225.744928   420.889234   \n",
       "std      57.361899    52.176096     8.047175    46.673734    57.745325   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%     249.000000   288.250000    39.530000   218.500000   402.562500   \n",
       "50%     262.000000   304.500000    42.735000   233.000000   426.500000   \n",
       "75%     274.000000   320.750000    45.762500   247.000000   447.812500   \n",
       "max     324.000000   379.500000    55.120000   290.250000   531.750000   \n",
       "\n",
       "               FBL          FHD          TML          FEB          TPB  \\\n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  1528.000000   \n",
       "mean    416.533704    42.939784   343.450524    74.060209    66.297120   \n",
       "std      59.293408     6.237500    63.587332    13.867850    15.272226   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%     398.937500    40.470000   330.687500    71.250000    64.500000   \n",
       "50%     422.625000    43.460000   352.500000    76.000000    69.000000   \n",
       "75%     444.250000    46.277500   372.000000    80.500000    73.500000   \n",
       "max     530.500000    57.625000   444.500000    93.250000    85.000000   \n",
       "\n",
       "               HEB          Sex  \n",
       "count  1528.000000  1528.000000  \n",
       "mean     56.201345     0.355366  \n",
       "std      10.868670     0.478781  \n",
       "min       0.000000     0.000000  \n",
       "25%      53.500000     0.000000  \n",
       "50%      57.500000     0.000000  \n",
       "75%      61.562500     1.000000  \n",
       "max      74.000000     1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the Sex column is a string, not a value, that's why\n",
    "# its not printed above. \n",
    "\n",
    "# But we take advandage of the fact that its a string to \n",
    "# drop the values 1? and 0?\n",
    "\n",
    "# Get rid of 1? and 0? from sex estimation and then shuffle the dataset\n",
    "# because otherwise you have 1 and 0 packed together \n",
    "\n",
    "model_data_goldman = pd.concat([model_data_goldman.loc[model_data_goldman['Sex']=='1'], model_data_goldman.loc[model_data_goldman['Sex']=='0']])\n",
    "\n",
    "model_data_goldman = model_data_goldman.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Now convert Sex from string to int\n",
    "\n",
    "model_data_goldman[\"Sex\"] = model_data_goldman[\"Sex\"].astype(int) \n",
    "\n",
    "model_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 0.0 back to nan to better handle the dataset within xgboost but\n",
    "# also to become able to drop the NA entries easily\n",
    "\n",
    "model_data_goldman[model_cols] = model_data_goldman[model_cols].replace(0.0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>HML</th>\n",
       "      <th>HHD</th>\n",
       "      <th>RML</th>\n",
       "      <th>FML</th>\n",
       "      <th>FBL</th>\n",
       "      <th>FHD</th>\n",
       "      <th>TML</th>\n",
       "      <th>FEB</th>\n",
       "      <th>TPB</th>\n",
       "      <th>HEB</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1491.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>1508.00000</td>\n",
       "      <td>1506.000000</td>\n",
       "      <td>1509.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1486.000000</td>\n",
       "      <td>1461.000000</td>\n",
       "      <td>1486.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>262.393763</td>\n",
       "      <td>305.569953</td>\n",
       "      <td>42.864694</td>\n",
       "      <td>233.856441</td>\n",
       "      <td>426.47132</td>\n",
       "      <td>422.618526</td>\n",
       "      <td>43.480444</td>\n",
       "      <td>352.920242</td>\n",
       "      <td>76.153432</td>\n",
       "      <td>69.337440</td>\n",
       "      <td>57.789808</td>\n",
       "      <td>0.355366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.299914</td>\n",
       "      <td>22.965932</td>\n",
       "      <td>4.148381</td>\n",
       "      <td>18.935423</td>\n",
       "      <td>31.56893</td>\n",
       "      <td>31.525125</td>\n",
       "      <td>3.984079</td>\n",
       "      <td>28.471105</td>\n",
       "      <td>6.184101</td>\n",
       "      <td>5.743871</td>\n",
       "      <td>5.441524</td>\n",
       "      <td>0.478781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>179.500000</td>\n",
       "      <td>343.75000</td>\n",
       "      <td>329.750000</td>\n",
       "      <td>33.315000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>251.000000</td>\n",
       "      <td>289.500000</td>\n",
       "      <td>39.805000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>404.00000</td>\n",
       "      <td>399.812500</td>\n",
       "      <td>40.635000</td>\n",
       "      <td>332.500000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>263.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>42.920000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>427.00000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>43.505000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>321.250000</td>\n",
       "      <td>45.810000</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>448.00000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>46.305000</td>\n",
       "      <td>372.250000</td>\n",
       "      <td>80.687500</td>\n",
       "      <td>73.750000</td>\n",
       "      <td>61.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>55.120000</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>531.75000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>57.625000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>93.250000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BIB          HML          HHD          RML         FML  \\\n",
       "count  1459.000000  1491.000000  1487.000000  1475.000000  1508.00000   \n",
       "mean    262.393763   305.569953    42.864694   233.856441   426.47132   \n",
       "std      18.299914    22.965932     4.148381    18.935423    31.56893   \n",
       "min     184.000000   234.000000    29.580000   179.500000   343.75000   \n",
       "25%     251.000000   289.500000    39.805000   220.000000   404.00000   \n",
       "50%     263.000000   305.000000    42.920000   234.000000   427.00000   \n",
       "75%     274.000000   321.250000    45.810000   247.500000   448.00000   \n",
       "max     324.000000   379.500000    55.120000   290.250000   531.75000   \n",
       "\n",
       "               FBL          FHD          TML          FEB          TPB  \\\n",
       "count  1506.000000  1509.000000  1487.000000  1486.000000  1461.000000   \n",
       "mean    422.618526    43.480444   352.920242    76.153432    69.337440   \n",
       "std      31.525125     3.984079    28.471105     6.184101     5.743871   \n",
       "min     329.750000    33.315000   271.000000    58.000000    50.000000   \n",
       "25%     399.812500    40.635000   332.500000    71.500000    65.000000   \n",
       "50%     423.000000    43.505000   353.000000    76.250000    69.500000   \n",
       "75%     444.500000    46.305000   372.250000    80.687500    73.750000   \n",
       "max     530.500000    57.625000   444.500000    93.250000    85.000000   \n",
       "\n",
       "               HEB          Sex  \n",
       "count  1486.000000  1528.000000  \n",
       "mean     57.789808     0.355366  \n",
       "std       5.441524     0.478781  \n",
       "min      41.500000     0.000000  \n",
       "25%      54.000000     0.000000  \n",
       "50%      58.000000     0.000000  \n",
       "75%      61.750000     1.000000  \n",
       "max      74.000000     1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_zeroes_model_data_goldman = model_data_goldman.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>HML</th>\n",
       "      <th>HHD</th>\n",
       "      <th>RML</th>\n",
       "      <th>FML</th>\n",
       "      <th>FBL</th>\n",
       "      <th>FHD</th>\n",
       "      <th>TML</th>\n",
       "      <th>FEB</th>\n",
       "      <th>TPB</th>\n",
       "      <th>HEB</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>262.400154</td>\n",
       "      <td>306.085292</td>\n",
       "      <td>42.924977</td>\n",
       "      <td>233.967358</td>\n",
       "      <td>427.142665</td>\n",
       "      <td>423.216590</td>\n",
       "      <td>43.548280</td>\n",
       "      <td>353.362058</td>\n",
       "      <td>76.298771</td>\n",
       "      <td>69.439324</td>\n",
       "      <td>57.842669</td>\n",
       "      <td>0.353303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.237396</td>\n",
       "      <td>22.684177</td>\n",
       "      <td>4.075788</td>\n",
       "      <td>18.760498</td>\n",
       "      <td>31.103027</td>\n",
       "      <td>31.078794</td>\n",
       "      <td>3.938374</td>\n",
       "      <td>28.230646</td>\n",
       "      <td>6.128510</td>\n",
       "      <td>5.709825</td>\n",
       "      <td>5.386024</td>\n",
       "      <td>0.478179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>31.650000</td>\n",
       "      <td>179.500000</td>\n",
       "      <td>343.750000</td>\n",
       "      <td>329.750000</td>\n",
       "      <td>33.315000</td>\n",
       "      <td>276.500000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>251.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>39.913750</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>401.062500</td>\n",
       "      <td>40.735000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>71.750000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>263.000000</td>\n",
       "      <td>305.375000</td>\n",
       "      <td>42.945000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>427.500000</td>\n",
       "      <td>423.500000</td>\n",
       "      <td>43.565000</td>\n",
       "      <td>353.375000</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>321.750000</td>\n",
       "      <td>45.825000</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>46.355000</td>\n",
       "      <td>372.250000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>61.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>54.760000</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>531.750000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>56.195000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>93.250000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BIB          HML          HHD          RML          FML  \\\n",
       "count  1302.000000  1302.000000  1302.000000  1302.000000  1302.000000   \n",
       "mean    262.400154   306.085292    42.924977   233.967358   427.142665   \n",
       "std      18.237396    22.684177     4.075788    18.760498    31.103027   \n",
       "min     184.000000   234.000000    31.650000   179.500000   343.750000   \n",
       "25%     251.000000   290.000000    39.913750   220.500000   405.000000   \n",
       "50%     263.000000   305.375000    42.945000   234.000000   427.500000   \n",
       "75%     274.000000   321.750000    45.825000   247.500000   448.000000   \n",
       "max     324.000000   379.500000    54.760000   290.250000   531.750000   \n",
       "\n",
       "               FBL          FHD          TML          FEB          TPB  \\\n",
       "count  1302.000000  1302.000000  1302.000000  1302.000000  1302.000000   \n",
       "mean    423.216590    43.548280   353.362058    76.298771    69.439324   \n",
       "std      31.078794     3.938374    28.230646     6.128510     5.709825   \n",
       "min     329.750000    33.315000   276.500000    58.000000    50.000000   \n",
       "25%     401.062500    40.735000   333.000000    71.750000    65.000000   \n",
       "50%     423.500000    43.565000   353.375000    76.500000    69.500000   \n",
       "75%     444.500000    46.355000   372.250000    81.000000    74.000000   \n",
       "max     530.500000    56.195000   444.500000    93.250000    85.000000   \n",
       "\n",
       "               HEB          Sex  \n",
       "count  1302.000000  1302.000000  \n",
       "mean     57.842669     0.353303  \n",
       "std       5.386024     0.478179  \n",
       "min      41.500000     0.000000  \n",
       "25%      54.000000     0.000000  \n",
       "50%      58.000000     0.000000  \n",
       "75%      61.750000     1.000000  \n",
       "max      74.000000     1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_zeroes_model_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data set using the knn imputer\n",
    "# Here we use the 3 nearest neighbors to calculate the missing data\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3, missing_values=0.0)\n",
    "\n",
    "sex_column = model_data_goldman['Sex']\n",
    "\n",
    "temporary_data_set = model_data_goldman.fillna(0.).drop([\"Sex\"],axis=1)\n",
    "\n",
    "cols = temporary_data_set.columns\n",
    "\n",
    "temporary_data_set = knn_imputer.fit_transform(temporary_data_set)\n",
    "\n",
    "temporary_data_set = pd.DataFrame(data=temporary_data_set, columns=cols)\n",
    "\n",
    "knn_imputed_data_goldman = pd.concat([temporary_data_set,sex_column],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>HML</th>\n",
       "      <th>HHD</th>\n",
       "      <th>RML</th>\n",
       "      <th>FML</th>\n",
       "      <th>FBL</th>\n",
       "      <th>FHD</th>\n",
       "      <th>TML</th>\n",
       "      <th>FEB</th>\n",
       "      <th>TPB</th>\n",
       "      <th>HEB</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>262.316099</td>\n",
       "      <td>305.536409</td>\n",
       "      <td>42.857743</td>\n",
       "      <td>233.813264</td>\n",
       "      <td>426.412195</td>\n",
       "      <td>422.530705</td>\n",
       "      <td>43.476048</td>\n",
       "      <td>352.991099</td>\n",
       "      <td>76.122055</td>\n",
       "      <td>69.324498</td>\n",
       "      <td>57.782715</td>\n",
       "      <td>0.355366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.134815</td>\n",
       "      <td>22.874768</td>\n",
       "      <td>4.118270</td>\n",
       "      <td>18.836392</td>\n",
       "      <td>31.490822</td>\n",
       "      <td>31.449916</td>\n",
       "      <td>3.972349</td>\n",
       "      <td>28.387212</td>\n",
       "      <td>6.159489</td>\n",
       "      <td>5.705714</td>\n",
       "      <td>5.399224</td>\n",
       "      <td>0.478781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>179.500000</td>\n",
       "      <td>343.750000</td>\n",
       "      <td>329.750000</td>\n",
       "      <td>33.315000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>251.000000</td>\n",
       "      <td>289.500000</td>\n",
       "      <td>39.820000</td>\n",
       "      <td>220.250000</td>\n",
       "      <td>404.187500</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>40.635000</td>\n",
       "      <td>332.500000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>263.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>42.900000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>426.750000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>43.495000</td>\n",
       "      <td>353.375000</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>321.250000</td>\n",
       "      <td>45.790000</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>46.291250</td>\n",
       "      <td>372.500000</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>73.750000</td>\n",
       "      <td>61.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>55.120000</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>531.750000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>57.625000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>93.250000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BIB          HML          HHD          RML          FML  \\\n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  1528.000000   \n",
       "mean    262.316099   305.536409    42.857743   233.813264   426.412195   \n",
       "std      18.134815    22.874768     4.118270    18.836392    31.490822   \n",
       "min     184.000000   234.000000    29.580000   179.500000   343.750000   \n",
       "25%     251.000000   289.500000    39.820000   220.250000   404.187500   \n",
       "50%     263.000000   305.000000    42.900000   234.000000   426.750000   \n",
       "75%     274.000000   321.250000    45.790000   247.500000   448.000000   \n",
       "max     324.000000   379.500000    55.120000   290.250000   531.750000   \n",
       "\n",
       "               FBL          FHD          TML          FEB          TPB  \\\n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  1528.000000   \n",
       "mean    422.530705    43.476048   352.991099    76.122055    69.324498   \n",
       "std      31.449916     3.972349    28.387212     6.159489     5.705714   \n",
       "min     329.750000    33.315000   271.000000    58.000000    50.000000   \n",
       "25%     400.000000    40.635000   332.500000    71.500000    65.000000   \n",
       "50%     423.000000    43.495000   353.375000    76.250000    69.500000   \n",
       "75%     444.500000    46.291250   372.500000    80.500000    73.750000   \n",
       "max     530.500000    57.625000   444.500000    93.250000    85.000000   \n",
       "\n",
       "               HEB          Sex  \n",
       "count  1528.000000  1528.000000  \n",
       "mean     57.782715     0.355366  \n",
       "std       5.399224     0.478781  \n",
       "min      41.500000     0.000000  \n",
       "25%      54.000000     0.000000  \n",
       "50%      58.000000     0.000000  \n",
       "75%      61.750000     1.000000  \n",
       "max      74.000000     1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_imputed_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data set using the iterative imputer\n",
    "\n",
    "iter_imputer = IterativeImputer(max_iter = 1000, missing_values=0.0)\n",
    "\n",
    "sex_column = model_data_goldman['Sex']\n",
    "\n",
    "temporary_data_set = model_data_goldman.fillna(0.).drop([\"Sex\"],axis=1)\n",
    "\n",
    "cols = temporary_data_set.columns\n",
    "\n",
    "temporary_data_set = iter_imputer.fit_transform(temporary_data_set)\n",
    "\n",
    "temporary_data_set = pd.DataFrame(data=temporary_data_set, columns=cols)\n",
    "\n",
    "iter_imputed_data_goldman = pd.concat([temporary_data_set,sex_column],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>HML</th>\n",
       "      <th>HHD</th>\n",
       "      <th>RML</th>\n",
       "      <th>FML</th>\n",
       "      <th>FBL</th>\n",
       "      <th>FHD</th>\n",
       "      <th>TML</th>\n",
       "      <th>FEB</th>\n",
       "      <th>TPB</th>\n",
       "      <th>HEB</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>262.210761</td>\n",
       "      <td>305.512210</td>\n",
       "      <td>42.849808</td>\n",
       "      <td>233.797870</td>\n",
       "      <td>426.437825</td>\n",
       "      <td>422.559829</td>\n",
       "      <td>43.479740</td>\n",
       "      <td>352.974216</td>\n",
       "      <td>76.129804</td>\n",
       "      <td>69.308226</td>\n",
       "      <td>57.781046</td>\n",
       "      <td>0.355366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.081781</td>\n",
       "      <td>22.863518</td>\n",
       "      <td>4.124843</td>\n",
       "      <td>18.879069</td>\n",
       "      <td>31.498944</td>\n",
       "      <td>31.452610</td>\n",
       "      <td>3.974301</td>\n",
       "      <td>28.378044</td>\n",
       "      <td>6.167874</td>\n",
       "      <td>5.733651</td>\n",
       "      <td>5.402456</td>\n",
       "      <td>0.478781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>179.500000</td>\n",
       "      <td>343.750000</td>\n",
       "      <td>329.750000</td>\n",
       "      <td>33.315000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>251.000000</td>\n",
       "      <td>289.500000</td>\n",
       "      <td>39.813750</td>\n",
       "      <td>220.250000</td>\n",
       "      <td>404.187500</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>40.635000</td>\n",
       "      <td>332.500000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>263.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>42.917500</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>426.750000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>43.495000</td>\n",
       "      <td>353.250000</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>321.062500</td>\n",
       "      <td>45.786250</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>46.297500</td>\n",
       "      <td>372.500000</td>\n",
       "      <td>80.562500</td>\n",
       "      <td>73.696409</td>\n",
       "      <td>61.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>55.120000</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>531.750000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>57.625000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>93.250000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BIB          HML          HHD          RML          FML  \\\n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  1528.000000   \n",
       "mean    262.210761   305.512210    42.849808   233.797870   426.437825   \n",
       "std      18.081781    22.863518     4.124843    18.879069    31.498944   \n",
       "min     184.000000   234.000000    29.580000   179.500000   343.750000   \n",
       "25%     251.000000   289.500000    39.813750   220.250000   404.187500   \n",
       "50%     263.000000   305.000000    42.917500   234.000000   426.750000   \n",
       "75%     274.000000   321.062500    45.786250   247.500000   448.000000   \n",
       "max     324.000000   379.500000    55.120000   290.250000   531.750000   \n",
       "\n",
       "               FBL          FHD          TML          FEB          TPB  \\\n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  1528.000000   \n",
       "mean    422.559829    43.479740   352.974216    76.129804    69.308226   \n",
       "std      31.452610     3.974301    28.378044     6.167874     5.733651   \n",
       "min     329.750000    33.315000   271.000000    58.000000    50.000000   \n",
       "25%     400.000000    40.635000   332.500000    71.500000    65.000000   \n",
       "50%     423.000000    43.495000   353.250000    76.250000    69.500000   \n",
       "75%     444.500000    46.297500   372.500000    80.562500    73.696409   \n",
       "max     530.500000    57.625000   444.500000    93.250000    85.000000   \n",
       "\n",
       "               HEB          Sex  \n",
       "count  1528.000000  1528.000000  \n",
       "mean     57.781046     0.355366  \n",
       "std       5.402456     0.478781  \n",
       "min      41.500000     0.000000  \n",
       "25%      54.000000     0.000000  \n",
       "50%      58.000000     0.000000  \n",
       "75%      61.750000     1.000000  \n",
       "max      74.000000     1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_imputed_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data\n",
    "\n",
    "no_zeroes_model_data_goldman = no_zeroes_model_data_goldman.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "knn_imputed_data_goldman = knn_imputed_data_goldman.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "iter_imputed_data_goldman = iter_imputed_data_goldman.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = [\n",
    "    no_zeroes_model_data_goldman, \n",
    "    knn_imputed_data_goldman, \n",
    "    iter_imputed_data_goldman\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_names = [\n",
    "    \"Logistic Regression\", \n",
    "    \"Decision Tree Classifier\", \n",
    "    \"Support Vector Machines\", \n",
    "    \"Gaussian Process Classifier\", \n",
    "    \"Gradient Boosting Classifier\", \n",
    "    \"Random Forest Classifier\",\n",
    "    \"Ada Boost Classifier\", \n",
    "    \"Extra Trees Classifier\", \n",
    "    \"Gaussian Naive Bayes\", \n",
    "    \"KNNeighbors Classifier\",\n",
    "    \"Linear Discriminant Analysis\", \n",
    "    \"Quadratic Discriminant Analysis\", \n",
    "    \"XGBClassifier\", \n",
    "    \"Light Gradient Boosting Classifier\"\n",
    "]\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    GradientBoostingClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    XGBClassifier(),\n",
    "    lgb.LGBMClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "dataset_scores_list = []\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    scores = []\n",
    "    \n",
    "    X = dataset.drop('Sex', axis = 1).values\n",
    "    y = dataset['Sex']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size=0.3, stratify=y)\n",
    "    \n",
    "    for name, clf in zip(classifier_names, classifiers):\n",
    "        run_score = []\n",
    "        \n",
    "        for i in range(20):\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)*100\n",
    "            run_score.append(score)\n",
    "            \n",
    "            avg_score = np.mean(run_score)\n",
    "                      \n",
    "        #print(run_score)    \n",
    "        scores.append(avg_score)\n",
    "                  \n",
    "    dataset_scores_list.append(scores)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[80.05115089514065,\n",
       "  82.55754475703324,\n",
       "  79.79539641943734,\n",
       "  89.00255754475705,\n",
       "  86.95652173913045,\n",
       "  88.37595907928389,\n",
       "  86.95652173913045,\n",
       "  88.26086956521739,\n",
       "  84.91048593350385,\n",
       "  85.67774936061377,\n",
       "  90.02557544757035,\n",
       "  89.51406649616368,\n",
       "  87.72378516624039,\n",
       "  87.97953964194376],\n",
       " [82.78867102396516,\n",
       "  80.67538126361656,\n",
       "  79.520697167756,\n",
       "  84.31372549019606,\n",
       "  86.86274509803921,\n",
       "  85.30501089324619,\n",
       "  85.62091503267973,\n",
       "  86.23093681917211,\n",
       "  84.96732026143789,\n",
       "  83.66013071895424,\n",
       "  87.36383442265793,\n",
       "  85.40305010893243,\n",
       "  86.27450980392157,\n",
       "  85.18518518518519],\n",
       " [83.66013071895424,\n",
       "  84.04139433551198,\n",
       "  83.00653594771241,\n",
       "  84.53159041394336,\n",
       "  90.16339869281046,\n",
       "  91.27450980392156,\n",
       "  87.14596949891069,\n",
       "  91.63398692810458,\n",
       "  89.10675381263619,\n",
       "  87.14596949891069,\n",
       "  90.41394335511981,\n",
       "  90.19607843137257,\n",
       "  91.28540305010894,\n",
       "  88.88888888888889]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(index=classifier_names)\n",
    "# results['name'] = names\n",
    "results['goldman_1'] = dataset_scores_list[0]\n",
    "results['goldman_2'] = dataset_scores_list[1]\n",
    "results['goldman_3'] = dataset_scores_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goldman_1</th>\n",
       "      <th>goldman_2</th>\n",
       "      <th>goldman_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>80.051151</td>\n",
       "      <td>82.788671</td>\n",
       "      <td>83.660131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>82.557545</td>\n",
       "      <td>80.675381</td>\n",
       "      <td>84.041394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machines</th>\n",
       "      <td>79.795396</td>\n",
       "      <td>79.520697</td>\n",
       "      <td>83.006536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Process Classifier</th>\n",
       "      <td>89.002558</td>\n",
       "      <td>84.313725</td>\n",
       "      <td>84.531590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting Classifier</th>\n",
       "      <td>86.956522</td>\n",
       "      <td>86.862745</td>\n",
       "      <td>90.163399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <td>88.375959</td>\n",
       "      <td>85.305011</td>\n",
       "      <td>91.274510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ada Boost Classifier</th>\n",
       "      <td>86.956522</td>\n",
       "      <td>85.620915</td>\n",
       "      <td>87.145969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees Classifier</th>\n",
       "      <td>88.260870</td>\n",
       "      <td>86.230937</td>\n",
       "      <td>91.633987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>84.910486</td>\n",
       "      <td>84.967320</td>\n",
       "      <td>89.106754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors Classifier</th>\n",
       "      <td>85.677749</td>\n",
       "      <td>83.660131</td>\n",
       "      <td>87.145969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis</th>\n",
       "      <td>90.025575</td>\n",
       "      <td>87.363834</td>\n",
       "      <td>90.413943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quadratic Discriminant Analysis</th>\n",
       "      <td>89.514066</td>\n",
       "      <td>85.403050</td>\n",
       "      <td>90.196078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>87.723785</td>\n",
       "      <td>86.274510</td>\n",
       "      <td>91.285403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Light Gradient Boosting Classifier</th>\n",
       "      <td>87.979540</td>\n",
       "      <td>85.185185</td>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    goldman_1  goldman_2  goldman_3\n",
       "Logistic Regression                 80.051151  82.788671  83.660131\n",
       "Decision Tree Classifier            82.557545  80.675381  84.041394\n",
       "Support Vector Machines             79.795396  79.520697  83.006536\n",
       "Gaussian Process Classifier         89.002558  84.313725  84.531590\n",
       "Gradient Boosting Classifier        86.956522  86.862745  90.163399\n",
       "Random Forest Classifier            88.375959  85.305011  91.274510\n",
       "Ada Boost Classifier                86.956522  85.620915  87.145969\n",
       "Extra Trees Classifier              88.260870  86.230937  91.633987\n",
       "Gaussian Naive Bayes                84.910486  84.967320  89.106754\n",
       "KNNeighbors Classifier              85.677749  83.660131  87.145969\n",
       "Linear Discriminant Analysis        90.025575  87.363834  90.413943\n",
       "Quadratic Discriminant Analysis     89.514066  85.403050  90.196078\n",
       "XGBClassifier                       87.723785  86.274510  91.285403\n",
       "Light Gradient Boosting Classifier  87.979540  85.185185  88.888889"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = iter_imputed_data_goldman.drop('Sex', axis = 1).values\n",
    "# y = iter_imputed_data_goldman['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8366013071895425"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression model\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=10, estimator=LogisticRegression(),\n",
       "              search_spaces={'C': array([0.010000, 0.012664, 0.016037, 0.020309, 0.025719, 0.032570,\n",
       "       0.041246, 0.052233, 0.066147, 0.083768, 0.106082, 0.134340,\n",
       "       0.170125, 0.215443, 0.272833, 0.345511, 0.437548, 0.554102,\n",
       "       0.701704, 0.888624, 1.125336, 1.425103, 1.804722, 2.285464,\n",
       "       2.894266, 3.665241, 4.641589, 5.878016, 7.443803, 9.426685,\n",
       "       11.937766, 15.117751, 19.144820, 24.244620, 30.702906, 38.881552,\n",
       "       49.238826, 62.355073, 78.965229, 100.000000]),\n",
       "                             'max_iter': [1000, 1500, 2000],\n",
       "                             'random_state': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
       "                                              11]})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the logistic regression model\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'C': np.logspace(-2,2,40),\n",
    "    'max_iter': [1000, 1500, 2000],\n",
    "    'random_state': [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "}\n",
    "\n",
    "clf  = BayesSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('C', 2.2854638641349907),\n",
       "             ('max_iter', 1500),\n",
       "             ('random_state', 3)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     89.76\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8300653594771242"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support vector machines\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=SVC(),\n",
       "                   param_distributions={'C': array([0.010000, 0.027826, 0.077426, 0.215443, 0.599484, 1.668101,\n",
       "       4.641589, 12.915497, 35.938137, 100.000000]),\n",
       "                                        'kernel': ['rbf', 'linear']})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Support Vevtor Machine model\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "parameters = {\n",
    "    'C': np.logspace(-2,2,10),\n",
    "    'kernel': ['rbf','linear']\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'linear', 'C': 4.6415888336127775}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     91.07\n"
     ]
    }
   ],
   "source": [
    "model = SVC(**clf.best_params_, probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8714596949891068"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kNN classifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=10, estimator=KNeighborsClassifier(),\n",
       "              search_spaces={'leaf_size': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                             'metric': ['euclidean', 'manhattan'],\n",
       "                             'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n",
       "                                             12, 13, 14, 15, 16, 17, 18, 19,\n",
       "                                             20],\n",
       "                             'weights': ['uniform', 'distance']})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the kNN classifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_neighbors': list(range(1,21)),\n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'metric' : ['euclidean', 'manhattan'],\n",
    "    'leaf_size': list(range(1,10))\n",
    "}\n",
    "\n",
    "clf  = BayesSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('leaf_size', 3),\n",
       "             ('metric', 'manhattan'),\n",
       "             ('n_neighbors', 13),\n",
       "             ('weights', 'distance')])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     89.54\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8910675381263616"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=GaussianNB(),\n",
       "             param_grid={'var_smoothing': array([1000.000000, 705.480231, 497.702356, 351.119173, 247.707636,\n",
       "       174.752840, 123.284674, 86.974900, 61.359073, 43.287613, 30.538555,\n",
       "       21.544347, 15.199111, 10.722672, 7.564633, 5.336699, 3.764936,\n",
       "       2.656088, 1.873817, 1.321941, 0.932603, 0.657933, 0.464159,\n",
       "       0.327455, 0.231013, 0.162975, 0.114976, 0.081113...\n",
       "       0.000001, 0.000001, 0.000001, 0.000000, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000, 0.000000])})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Gaussian Naive Bayes classifier\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "parameters = {\n",
    "    'var_smoothing': np.logspace(3,-12, num=100)\n",
    "             }\n",
    "\n",
    "clf  = GridSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'var_smoothing': 7.56463327554629e-05}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     88.89\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9041394335511983"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=10, estimator=LinearDiscriminantAnalysis(),\n",
       "              search_spaces={'solver': ['svd', 'lsqr', 'eigen']})"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Linear Discriminant Analysis classifier\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "parameters = {\n",
    "    'solver' : ['svd', 'lsqr', 'eigen']\n",
    "}\n",
    "\n",
    "clf  = BayesSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('solver', 'eigen')])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     90.41\n"
     ]
    }
   ],
   "source": [
    "model = LinearDiscriminantAnalysis(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"lda_model_20220522.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9019607843137255"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quadratic Discriminant Analysis\n",
    "\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=QuadraticDiscriminantAnalysis(),\n",
       "             param_grid={'reg_param': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Quadratic Discriminant Analysis classifier\n",
    "\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "parameters = {\n",
    "    'reg_param' : [0., 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "clf  = GridSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_param': 0.3}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     90.20\n"
     ]
    }
   ],
   "source": [
    "model = QuadraticDiscriminantAnalysis(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8474945533769063"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'criterion': ['entropy', 'gini'],\n",
       "                         'max_depth': [1, 2, 3, 4, 5, 6, 7, 15, 20, 30, 40, 120,\n",
       "                                       150]})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Decision Tree Classifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'criterion':['entropy','gini'],\n",
    "    'max_depth':[1,2,3,4,5,6,7,15,20,30,40,120,150]\n",
    "}\n",
    "\n",
    "clf  = GridSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 5}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     89.32\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9150326797385621"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=RandomForestClassifier(), n_iter=20,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Random Forest Classifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, param_distributions=random_grid, n_iter = 20, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': None,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.93899782135077\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9128540305010894"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost Classifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=XGBClassifier(),\n",
       "                   param_distributions={'colsample_bytree': [0.6, 0.8, 1.0],\n",
       "                                        'gamma': [0.1, 0.5, 1, 1.5, 2, 5, 10],\n",
       "                                        'max_depth': [3, 4, 5, 5, 6, 7, 8, 9,\n",
       "                                                      10, 20, 30, 40, 50],\n",
       "                                        'min_child_weight': [1, 2, 3, 4, 5, 6,\n",
       "                                                             25, 50, 75, 100],\n",
       "                                        'n_estimators': [100, 200, 300, 500,\n",
       "                                                         800, 1000, 2000],\n",
       "                                        'subsample': [0.6, 0.8, 1.0]})"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the XGBoost Classifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'min_child_weight': [1, 2, 3, 4, 5, 6, 25, 50, 75, 100],\n",
    "    'gamma': [0.1, 0.5, 1, 1.5, 2, 5, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 4, 5, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50],\n",
    "    'n_estimators': [100, 200, 300, 500, 800, 1000, 2000]\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 1.0,\n",
       " 'n_estimators': 800,\n",
       " 'min_child_weight': 50,\n",
       " 'max_depth': 9,\n",
       " 'gamma': 1,\n",
       " 'colsample_bytree': 1.0}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9150326797385621\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"xgb_model_20220522.dat\", \"wb\"))\n",
    "\n",
    "# iter_imputed_data_goldman.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8453159041394336"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Process Classifier\n",
    "\n",
    "model = GaussianProcessClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 127-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 128-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 128-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 128-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 128-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 128-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 128-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 128-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 128-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 128-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.831136 nan 0.830478 0.884176 0.644634]\n",
      "  warnings.warn(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=GaussianProcessClassifier(),\n",
       "                   param_distributions={'kernel': [1**2 * RBF(length_scale=1),\n",
       "                                                   1**2 * DotProduct(sigma_0=1),\n",
       "                                                   1**2 * Matern(length_scale=1, nu=1.5),\n",
       "                                                   1**2 * RationalQuadratic(alpha=1, length_scale=1),\n",
       "                                                   1**2 * WhiteKernel(noise_level=1)]})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the GaussianProcessClassifier\n",
    "\n",
    "model = GaussianProcessClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'kernel' : [1*RBF(), 1*DotProduct(), 1*Matern(),  1*RationalQuadratic(), 1*WhiteKernel()]\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 1**2 * RationalQuadratic(alpha=1, length_scale=1)}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9106753812636166\n"
     ]
    }
   ],
   "source": [
    "model = GaussianProcessClassifier(**clf.best_params_, max_iter_predict = 1000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9019607843137255"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=10, estimator=GradientBoostingClassifier(),\n",
       "              search_spaces={'learning_rate': [0.01, 0.1, 1, 10, 100],\n",
       "                             'max_depth': [1, 3, 5, 7, 9],\n",
       "                             'n_estimators': [5, 50, 250, 500]})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Gradient Boosting Classifier\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\":[5,50,250,500],\n",
    "    \"max_depth\":[1,3,5,7,9],\n",
    "    \"learning_rate\":[0.01,0.1,1,10,100]\n",
    "}\n",
    "\n",
    "clf  = BayesSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('learning_rate', 0.1), ('max_depth', 9), ('n_estimators', 500)])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8714596949891068"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ada Boost Classifier\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:561: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(estimator_weight *\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sample_weight /= sample_weight_sum\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 130, in fit\n",
      "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 298, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, DOUBLE)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1376, in _check_sample_weight\n",
      "    sample_weight = check_array(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 720, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan 0.143314 0.143314 0.863893 0.871745 nan 0.143314 0.870438 0.862590\n",
      " 0.853414]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=AdaBoostClassifier(),\n",
       "                   param_distributions={'learning_rate': [0.01, 0.1, 1, 10,\n",
       "                                                          100],\n",
       "                                        'n_estimators': [5, 50, 250, 500]})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Gradient Boosting Classifier\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\":[5,50,250,500],\n",
    "    \"learning_rate\":[0.01,0.1,1,10,100]\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 500, 'learning_rate': 0.01}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9041394335511983\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(model, open(\"ada_boost_model.dat\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9084967320261438"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extra trees classifier\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.873697 0.880904 0.880242 0.882220 0.883518 0.886137 0.880246 0.872390\n",
      " 0.869784 nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=ExtraTreesClassifier(),\n",
       "                   param_distributions={'min_samples_leaf': [1, 2, 3, 4, 5, 6,\n",
       "                                                             7, 8, 9, 10, 11,\n",
       "                                                             12, 13, 14, 15, 16,\n",
       "                                                             17, 18, 19],\n",
       "                                        'min_samples_split': [1, 2, 3, 4, 5, 6,\n",
       "                                                              7, 8, 9, 10, 11,\n",
       "                                                              12, 13, 14, 15,\n",
       "                                                              16, 17, 18, 19],\n",
       "                                        'n_estimators': [50, 75, 100, 125]})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the SGDClassifier\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators': list(range(50,126,25)),\n",
    "    'min_samples_leaf': list(range(1,20,1)),\n",
    "    'min_samples_split': list(range(1,20,1))\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 125, 'min_samples_split': 4, 'min_samples_leaf': 4}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9128540305010894\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Light boosting regressor\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=LGBMClassifier(),\n",
       "                   param_distributions={'min_child_samples': [20, 30, 50, 100,\n",
       "                                                              200, 300, 400],\n",
       "                                        'min_child_weight': [1e-05, 0.001, 0.01,\n",
       "                                                             0.1, 1, 10, 100],\n",
       "                                        'num_leaves': [5, 10, 20, 31, 50, 100,\n",
       "                                                       125, 150, 200],\n",
       "                                        'reg_alpha': [0, 0.1, 1, 2, 3, 5, 10],\n",
       "                                        'reg_lambda': [0, 0.1, 1, 5, 10, 20, 40,\n",
       "                                                       50]})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the SGDClassifier\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'num_leaves': [5, 10, 20, 31, 50, 100, 125, 150, 200], \n",
    "    'min_child_samples': [20, 30, 50 , 100, 200, 300, 400], \n",
    "    'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
    "    'reg_alpha': [0, 1e-1, 1, 2, 3, 5, 10],\n",
    "    'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 40, 50]\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_lambda': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'num_leaves': 125,\n",
       " 'min_child_weight': 0.1,\n",
       " 'min_child_samples': 300}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9128540305010894\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"lgb_model_20220522.dat\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_BIB_.dat\n",
      "0.6579520697167756\n",
      "model_lda_goldman_HML_.dat\n",
      "0.7668845315904139\n",
      "model_lda_goldman_HHD_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_RML_.dat\n",
      "0.7821350762527233\n",
      "model_lda_goldman_FML_.dat\n",
      "0.775599128540305\n",
      "model_lda_goldman_FBL_.dat\n",
      "0.7581699346405228\n",
      "model_lda_goldman_FHD_.dat\n",
      "0.7864923747276689\n",
      "model_lda_goldman_TML_.dat\n",
      "0.7276688453159041\n",
      "model_lda_goldman_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_BIB_HML_.dat\n",
      "0.7559912854030502\n",
      "model_lda_goldman_BIB_HHD_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_RML_.dat\n",
      "0.7864923747276689\n",
      "model_lda_goldman_BIB_FML_.dat\n",
      "0.7647058823529411\n",
      "model_lda_goldman_BIB_FBL_.dat\n",
      "0.775599128540305\n",
      "model_lda_goldman_BIB_FHD_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_BIB_TML_.dat\n",
      "0.7494553376906318\n",
      "model_lda_goldman_BIB_FEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_BIB_TPB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_BIB_HEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HML_HHD_.dat\n",
      "0.8235294117647058\n",
      "model_lda_goldman_HML_RML_.dat\n",
      "0.7995642701525054\n",
      "model_lda_goldman_HML_FML_.dat\n",
      "0.7625272331154684\n",
      "model_lda_goldman_HML_FBL_.dat\n",
      "0.7777777777777778\n",
      "model_lda_goldman_HML_FHD_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_HML_TML_.dat\n",
      "0.7799564270152506\n",
      "model_lda_goldman_HML_FEB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_HML_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HEB_.dat\n",
      "0.8257080610021786\n",
      "model_lda_goldman_HHD_RML_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_FML_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_FBL_.dat\n",
      "0.8169934640522876\n",
      "model_lda_goldman_HHD_FHD_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HHD_TML_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_FEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_TPB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HHD_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_RML_FML_.dat\n",
      "0.7930283224400871\n",
      "model_lda_goldman_RML_FBL_.dat\n",
      "0.7864923747276689\n",
      "model_lda_goldman_RML_FHD_.dat\n",
      "0.8235294117647058\n",
      "model_lda_goldman_RML_TML_.dat\n",
      "0.7973856209150327\n",
      "model_lda_goldman_RML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_RML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_FML_FBL_.dat\n",
      "0.7516339869281046\n",
      "model_lda_goldman_FML_FHD_.dat\n",
      "0.8169934640522876\n",
      "model_lda_goldman_FML_TML_.dat\n",
      "0.7538126361655774\n",
      "model_lda_goldman_FML_FEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_FML_TPB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_FML_HEB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_FBL_FHD_.dat\n",
      "0.8257080610021786\n",
      "model_lda_goldman_FBL_TML_.dat\n",
      "0.7690631808278867\n",
      "model_lda_goldman_FBL_FEB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_FBL_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_FBL_HEB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_FHD_TML_.dat\n",
      "0.8235294117647058\n",
      "model_lda_goldman_FHD_FEB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_FHD_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_FHD_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_TML_FEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_TML_TPB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_TML_HEB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_FEB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_TPB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_HML_HHD_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_HML_RML_.dat\n",
      "0.7973856209150327\n",
      "model_lda_goldman_BIB_HML_FML_.dat\n",
      "0.7516339869281046\n",
      "model_lda_goldman_BIB_HML_FBL_.dat\n",
      "0.7952069716775599\n",
      "model_lda_goldman_BIB_HML_FHD_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_BIB_HML_TML_.dat\n",
      "0.738562091503268\n",
      "model_lda_goldman_BIB_HML_FEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_TPB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_BIB_HML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_HHD_RML_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_FML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_FBL_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_HHD_FHD_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_TML_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_FEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_RML_FML_.dat\n",
      "0.7886710239651417\n",
      "model_lda_goldman_BIB_RML_FBL_.dat\n",
      "0.7973856209150327\n",
      "model_lda_goldman_BIB_RML_FHD_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_RML_TML_.dat\n",
      "0.8017429193899782\n",
      "model_lda_goldman_BIB_RML_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_RML_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_RML_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_FML_FBL_.dat\n",
      "0.7559912854030502\n",
      "model_lda_goldman_BIB_FML_FHD_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_FML_TML_.dat\n",
      "0.7712418300653595\n",
      "model_lda_goldman_BIB_FML_FEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_BIB_FML_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_FML_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_BIB_FBL_FHD_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_FBL_TML_.dat\n",
      "0.7559912854030502\n",
      "model_lda_goldman_BIB_FBL_FEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_FBL_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_FBL_HEB_.dat\n",
      "0.8322440087145969\n",
      "model_lda_goldman_BIB_FHD_TML_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_FHD_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_FHD_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_FHD_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_TML_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_TML_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_TML_HEB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_BIB_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_HHD_RML_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_FML_.dat\n",
      "0.8235294117647058\n",
      "model_lda_goldman_HML_HHD_FBL_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_HML_HHD_FHD_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_HHD_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_HHD_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FML_.dat\n",
      "0.7930283224400871\n",
      "model_lda_goldman_HML_RML_FBL_.dat\n",
      "0.7843137254901961\n",
      "model_lda_goldman_HML_RML_FHD_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HML_RML_TML_.dat\n",
      "0.8126361655773421\n",
      "model_lda_goldman_HML_RML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_TPB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_RML_HEB_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_HML_FML_FBL_.dat\n",
      "0.7712418300653595\n",
      "model_lda_goldman_HML_FML_FHD_.dat\n",
      "0.8191721132897604\n",
      "model_lda_goldman_HML_FML_TML_.dat\n",
      "0.7516339869281046\n",
      "model_lda_goldman_HML_FML_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_FML_TPB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HML_FML_HEB_.dat\n",
      "0.8235294117647058\n",
      "model_lda_goldman_HML_FBL_FHD_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_FBL_TML_.dat\n",
      "0.7821350762527233\n",
      "model_lda_goldman_HML_FBL_FEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_FBL_TPB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_HML_FBL_HEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HML_FHD_TML_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_HML_FHD_FEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_FHD_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_FHD_HEB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_HML_TML_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_TML_TPB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HML_TML_HEB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_HML_FEB_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FML_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_FBL_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FHD_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_TML_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_RML_FEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_RML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HHD_FML_FBL_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_FML_FHD_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HHD_FML_TML_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_FML_FEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HHD_FML_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_FML_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_FBL_FHD_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HHD_FBL_TML_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_FBL_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_FBL_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_FBL_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_FHD_TML_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_FHD_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_FHD_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HHD_FHD_HEB_.dat\n",
      "0.8474945533769063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_HHD_TML_FEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HHD_TML_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_TML_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_RML_FML_FBL_.dat\n",
      "0.8082788671023965\n",
      "model_lda_goldman_RML_FML_FHD_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_RML_FML_TML_.dat\n",
      "0.7777777777777778\n",
      "model_lda_goldman_RML_FML_FEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_RML_FML_TPB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_RML_FML_HEB_.dat\n",
      "0.8322440087145969\n",
      "model_lda_goldman_RML_FBL_FHD_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_RML_FBL_TML_.dat\n",
      "0.7952069716775599\n",
      "model_lda_goldman_RML_FBL_FEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_RML_FBL_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_RML_FBL_HEB_.dat\n",
      "0.8191721132897604\n",
      "model_lda_goldman_RML_FHD_TML_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_RML_FHD_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_RML_FHD_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_RML_FHD_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_RML_TML_FEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_RML_TML_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_RML_TML_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_RML_FEB_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_RML_FEB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_RML_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_FML_FBL_FHD_.dat\n",
      "0.8278867102396514\n",
      "model_lda_goldman_FML_FBL_TML_.dat\n",
      "0.7581699346405228\n",
      "model_lda_goldman_FML_FBL_FEB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_FML_FBL_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_FML_FBL_HEB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_FML_FHD_TML_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_FML_FHD_FEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_FML_FHD_TPB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_FML_FHD_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_FML_TML_FEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_FML_TML_TPB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_FML_TML_HEB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_FML_FEB_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_FML_FEB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_FML_TPB_HEB_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_FBL_FHD_TML_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_FBL_FHD_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_FBL_FHD_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_FBL_FHD_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_FBL_TML_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_FBL_TML_TPB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_FBL_TML_HEB_.dat\n",
      "0.8257080610021786\n",
      "model_lda_goldman_FBL_FEB_TPB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_FBL_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_FBL_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_FHD_TML_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_FHD_TML_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_FHD_TML_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_FHD_FEB_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_FHD_FEB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_FHD_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_TML_FEB_TPB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_TML_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_FEB_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_HML_HHD_RML_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_HHD_FML_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_HHD_TML_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_HHD_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_HHD_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_HML_RML_FML_.dat\n",
      "0.7886710239651417\n",
      "model_lda_goldman_BIB_HML_RML_FBL_.dat\n",
      "0.7647058823529411\n",
      "model_lda_goldman_BIB_HML_RML_FHD_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_RML_TML_.dat\n",
      "0.8191721132897604\n",
      "model_lda_goldman_BIB_HML_RML_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_RML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_RML_HEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_BIB_HML_FML_FBL_.dat\n",
      "0.7734204793028322\n",
      "model_lda_goldman_BIB_HML_FML_FHD_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_FML_TML_.dat\n",
      "0.7712418300653595\n",
      "model_lda_goldman_BIB_HML_FML_FEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_FML_TPB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_BIB_HML_FML_HEB_.dat\n",
      "0.8278867102396514\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_BIB_HML_FBL_TML_.dat\n",
      "0.7952069716775599\n",
      "model_lda_goldman_BIB_HML_FBL_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_FBL_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_FBL_HEB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_BIB_HML_FHD_TML_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_BIB_HML_FHD_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_FHD_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_FHD_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_TML_FEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_TML_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_TML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_HML_FEB_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HHD_RML_FML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HHD_RML_TML_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HHD_RML_FEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_RML_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_RML_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HHD_FML_TML_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HHD_FML_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_FML_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_FML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HHD_FBL_TML_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_FBL_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HHD_FBL_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_FBL_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_FHD_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_FHD_FEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_FHD_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HHD_FHD_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HHD_TML_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_TML_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_TML_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HHD_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HHD_FEB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_RML_FML_FBL_.dat\n",
      "0.7930283224400871\n",
      "model_lda_goldman_BIB_RML_FML_FHD_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_RML_FML_TML_.dat\n",
      "0.8104575163398693\n",
      "model_lda_goldman_BIB_RML_FML_FEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_RML_FML_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_RML_FML_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_RML_FBL_TML_.dat\n",
      "0.8082788671023965\n",
      "model_lda_goldman_BIB_RML_FBL_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_RML_FBL_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_RML_FBL_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_RML_FHD_TML_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_RML_FHD_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_RML_FHD_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_RML_FHD_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_RML_TML_FEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_RML_TML_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_RML_TML_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_RML_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_RML_FEB_HEB_.dat\n",
      "0.8605664488017429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_BIB_RML_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_FML_FBL_TML_.dat\n",
      "0.738562091503268\n",
      "model_lda_goldman_BIB_FML_FBL_FEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_BIB_FML_FBL_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_FML_FBL_HEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_BIB_FML_FHD_TML_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_BIB_FML_FHD_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_FML_FHD_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_FML_FHD_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_FML_TML_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_FML_TML_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_FML_TML_HEB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_BIB_FML_FEB_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_FML_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_FML_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_FBL_FHD_TML_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_FBL_FHD_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_FBL_FHD_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_FBL_FHD_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_FBL_TML_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_FBL_TML_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_FBL_TML_HEB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_BIB_FBL_FEB_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_FBL_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_FBL_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_FHD_TML_FEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_FHD_TML_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_FHD_TML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_FHD_FEB_TPB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_FHD_FEB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_FHD_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_TML_FEB_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_TML_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_TML_TPB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_FEB_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_RML_FML_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_RML_FBL_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_RML_FHD_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_RML_TML_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_HML_HHD_RML_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_HHD_RML_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_RML_HEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HML_HHD_FML_FBL_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_HML_HHD_FML_FHD_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_FML_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_FML_FEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_FML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_FML_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_FBL_TML_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_FBL_FEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_FBL_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_FBL_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HML_HHD_FHD_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_FHD_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_FHD_TPB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_HML_HHD_FHD_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_TML_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_TML_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_HHD_TML_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_FEB_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_FEB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_HHD_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_RML_FML_FBL_.dat\n",
      "0.8082788671023965\n",
      "model_lda_goldman_HML_RML_FML_FHD_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_RML_FML_TML_.dat\n",
      "0.8191721132897604\n",
      "model_lda_goldman_HML_RML_FML_FEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_RML_FML_TPB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HML_RML_FML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_RML_FBL_FHD_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_FBL_TML_.dat\n",
      "0.8191721132897604\n",
      "model_lda_goldman_HML_RML_FBL_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_RML_FBL_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_RML_FBL_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_RML_FHD_TML_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HML_RML_FHD_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HML_RML_FHD_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_RML_FHD_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_TML_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_RML_TML_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_RML_TML_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_RML_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_FEB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HML_RML_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_FML_FBL_FHD_.dat\n",
      "0.8322440087145969\n",
      "model_lda_goldman_HML_FML_FBL_TML_.dat\n",
      "0.7668845315904139\n",
      "model_lda_goldman_HML_FML_FBL_FEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_FML_FBL_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FML_FBL_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_FML_FHD_TML_.dat\n",
      "0.8061002178649237\n",
      "model_lda_goldman_HML_FML_FHD_FEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_FML_FHD_TPB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_HML_FML_FHD_HEB_.dat\n",
      "0.8235294117647058\n",
      "model_lda_goldman_HML_FML_TML_FEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HML_FML_TML_TPB_.dat\n",
      "0.8213507625272332\n",
      "model_lda_goldman_HML_FML_TML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_FML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_FML_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_FML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_FBL_FHD_TML_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_HML_FBL_FHD_FEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_FBL_FHD_TPB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_FBL_FHD_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FBL_TML_FEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_FBL_TML_TPB_.dat\n",
      "0.8278867102396514\n",
      "model_lda_goldman_HML_FBL_TML_HEB_.dat\n",
      "0.8126361655773421\n",
      "model_lda_goldman_HML_FBL_FEB_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FBL_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_FBL_TPB_HEB_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_HML_FHD_TML_FEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FHD_TML_TPB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HML_FHD_TML_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_FHD_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_FHD_FEB_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HML_FHD_TPB_HEB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_HML_TML_FEB_TPB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HML_TML_FEB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_TML_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_FEB_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_RML_FML_FBL_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_RML_FML_FHD_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HHD_RML_FML_TML_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_RML_FML_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_HHD_RML_FML_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HHD_RML_FML_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_RML_FBL_TML_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_RML_FBL_FEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_RML_FBL_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_RML_FBL_HEB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_HHD_RML_FHD_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HHD_RML_FHD_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_FHD_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HHD_RML_FHD_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HHD_RML_TML_FEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_RML_TML_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_RML_TML_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_RML_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_RML_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_RML_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_HHD_FML_FBL_TML_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_FML_FBL_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HHD_FML_FBL_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_HHD_FML_FBL_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_HHD_FML_FHD_TML_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_HHD_FML_FHD_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_FML_FHD_TPB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HHD_FML_FHD_HEB_.dat\n",
      "0.8169934640522876\n",
      "model_lda_goldman_HHD_FML_TML_FEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_FML_TML_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_FML_TML_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HHD_FML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_FML_FEB_HEB_.dat\n",
      "0.8627450980392157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_HHD_FML_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HHD_FBL_FHD_TML_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_FBL_FHD_FEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HHD_FBL_FHD_TPB_.dat\n",
      "0.8322440087145969\n",
      "model_lda_goldman_HHD_FBL_FHD_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HHD_FBL_TML_FEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_FBL_TML_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_FBL_TML_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_FBL_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HHD_FBL_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_FBL_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HHD_FHD_TML_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_FHD_TML_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_FHD_TML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HHD_FHD_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HHD_FHD_FEB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_FHD_TPB_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HHD_TML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_TML_FEB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_TML_TPB_HEB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_HHD_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_RML_FML_FBL_FHD_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_RML_FML_FBL_TML_.dat\n",
      "0.8104575163398693\n",
      "model_lda_goldman_RML_FML_FBL_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_RML_FML_FBL_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_RML_FML_FBL_HEB_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_RML_FML_FHD_TML_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_RML_FML_FHD_FEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_RML_FML_FHD_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_RML_FML_FHD_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_RML_FML_TML_FEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_RML_FML_TML_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_RML_FML_TML_HEB_.dat\n",
      "0.8278867102396514\n",
      "model_lda_goldman_RML_FML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_FML_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_FML_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_RML_FBL_FHD_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FBL_FHD_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_FBL_FHD_TPB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_RML_FBL_FHD_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_RML_FBL_TML_FEB_.dat\n",
      "0.906318082788671\n",
      "model_lda_goldman_RML_FBL_TML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_RML_FBL_TML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_RML_FBL_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_RML_FBL_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_RML_FBL_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_FHD_TML_FEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_RML_FHD_TML_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_RML_FHD_TML_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_RML_FHD_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_RML_FHD_FEB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_RML_FHD_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_RML_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_RML_TML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_FML_FBL_FHD_TML_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_FML_FBL_FHD_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_FML_FBL_FHD_TPB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_FML_FBL_FHD_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_FML_FBL_TML_FEB_.dat\n",
      "0.8322440087145969\n",
      "model_lda_goldman_FML_FBL_TML_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_FML_FBL_TML_HEB_.dat\n",
      "0.8104575163398693\n",
      "model_lda_goldman_FML_FBL_FEB_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_FML_FBL_FEB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_FML_FBL_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_FML_FHD_TML_FEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_FML_FHD_TML_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_FML_FHD_TML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_FML_FHD_FEB_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_FML_FHD_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_FML_FHD_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_FML_TML_FEB_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_FML_TML_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_FML_TML_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_FML_FEB_TPB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_FBL_FHD_TML_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_FBL_FHD_TML_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_FBL_FHD_TML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_FBL_FHD_FEB_TPB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_FBL_FHD_FEB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_FBL_FHD_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_FBL_TML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_FBL_TML_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_FBL_TML_TPB_HEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_FBL_FEB_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_FHD_TML_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_FHD_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_FHD_TML_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_FHD_FEB_TPB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_TML_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_BIB_HML_HHD_RML_TML_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_HHD_RML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_HHD_RML_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_BIB_HML_HHD_FML_TML_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HML_HHD_FML_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_FML_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_TML_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FEB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_TML_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_FEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_TPB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_HHD_TML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_TML_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_TML_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_HHD_FEB_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_HHD_FEB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_.dat\n",
      "0.7952069716775599\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_FML_TML_.dat\n",
      "0.8148148148148148\n",
      "model_lda_goldman_BIB_HML_RML_FML_FEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FML_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_RML_FML_HEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_RML_FBL_TML_.dat\n",
      "0.7908496732026143\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_RML_FBL_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_RML_FBL_HEB_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_BIB_HML_RML_FHD_TML_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_FHD_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_RML_FHD_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_FHD_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_TML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_RML_TML_TPB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_BIB_HML_RML_TML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_HML_RML_FEB_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_RML_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_HML_FML_FBL_TML_.dat\n",
      "0.7690631808278867\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FEB_.dat\n",
      "0.8714596949891068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_BIB_HML_FML_FBL_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_FML_FBL_HEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_BIB_HML_FML_FHD_TML_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_FML_FHD_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_FML_FHD_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_HML_FML_FHD_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_FML_TML_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_FML_TML_TPB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_BIB_HML_FML_TML_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_BIB_HML_FML_FEB_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HML_FML_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_FML_TPB_HEB_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_TML_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_FEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_FBL_TML_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_FBL_TML_TPB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_HML_FBL_TML_HEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_BIB_HML_FBL_FEB_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_FBL_FEB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_HML_FBL_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_FHD_TML_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_FHD_TML_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_FHD_TML_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_FHD_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_FHD_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_FHD_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_TML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_FEB_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_RML_FML_TML_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HHD_RML_FML_TPB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HHD_RML_FML_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_TML_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_TML_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_FEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_RML_TML_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_RML_TML_TPB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HHD_RML_TML_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FEB_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_RML_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_TML_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_TML_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_FEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HHD_FML_TML_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_FML_TML_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_FML_TML_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_FML_FEB_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_FML_FEB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_BIB_HHD_FML_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_TML_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_FEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HHD_FBL_TML_FEB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HHD_FBL_TML_TPB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_BIB_HHD_FBL_TML_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_FBL_FEB_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_FBL_FEB_HEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HHD_FBL_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_FHD_TML_FEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HHD_FHD_TML_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_FHD_TML_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_FHD_FEB_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_FHD_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_FHD_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_TML_FEB_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_TML_FEB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_TML_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_FEB_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_RML_FML_FBL_TML_.dat\n",
      "0.7973856209150327\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FEB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_RML_FML_FBL_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_RML_FML_FBL_HEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_BIB_RML_FML_FHD_TML_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_RML_FML_FHD_FEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_RML_FML_FHD_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_RML_FML_FHD_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_RML_FML_TML_FEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_RML_FML_TML_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_RML_FML_TML_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_RML_FML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_RML_FML_FEB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_RML_FML_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_TML_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_FEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_BIB_RML_FBL_TML_FEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_RML_FBL_TML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_RML_FBL_TML_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_RML_FBL_FEB_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_RML_FBL_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_RML_FBL_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_RML_FHD_TML_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_RML_FHD_TML_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_RML_FHD_TML_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_RML_FHD_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_RML_FHD_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_RML_FHD_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_RML_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_RML_TML_FEB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_RML_TML_TPB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_RML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_TML_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_TPB_.dat\n",
      "0.9041394335511983\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_FML_FBL_TML_FEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_FML_FBL_TML_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_FML_FBL_TML_HEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_BIB_FML_FBL_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_FML_FBL_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_FML_FBL_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_FML_FHD_TML_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_FML_FHD_TML_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_FML_FHD_TML_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_BIB_FML_FHD_FEB_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_FML_FHD_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_FML_FHD_TPB_HEB_.dat\n",
      "0.9084967320261438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_BIB_FML_TML_FEB_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_FML_TML_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_FML_TML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_FML_FEB_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_FBL_FHD_TML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_FBL_FHD_TML_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_FBL_FHD_TML_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_FBL_FHD_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_FBL_FHD_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_FBL_FHD_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_FBL_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_FBL_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_FBL_TML_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_FBL_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_FHD_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_FHD_TML_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_FHD_TML_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_FHD_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_RML_FML_TML_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_RML_FML_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_HML_HHD_RML_FML_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_RML_FML_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_RML_FBL_TML_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_RML_FBL_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_RML_FBL_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_RML_FHD_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_RML_FHD_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_RML_FHD_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_HHD_RML_FHD_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_HHD_RML_TML_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_RML_TML_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_RML_TML_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_RML_FEB_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_RML_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_RML_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_HHD_FML_FBL_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HML_HHD_FML_FBL_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_HML_HHD_FML_FBL_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_FML_FHD_TML_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HML_HHD_FML_FHD_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_FML_FHD_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_FML_FHD_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_HHD_FML_TML_FEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_HHD_FML_TML_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_FML_TML_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_HHD_FML_FEB_TPB_.dat\n",
      "0.8213507625272332\n",
      "model_lda_goldman_HML_HHD_FML_FEB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_HHD_FML_TPB_HEB_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_TML_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_TPB_.dat\n",
      "0.8257080610021786\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_FBL_TML_FEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_FBL_TML_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_FBL_TML_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_FBL_FEB_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_HHD_FBL_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_FBL_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_FHD_TML_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HML_HHD_FHD_TML_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_HHD_FHD_TML_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HML_HHD_FHD_FEB_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_FHD_FEB_HEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HML_HHD_FHD_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_TML_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_TML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_HHD_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FML_FBL_TML_.dat\n",
      "0.8017429193899782\n",
      "model_lda_goldman_HML_RML_FML_FBL_FEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_RML_FML_FBL_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_RML_FML_FBL_HEB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_HML_RML_FML_FHD_TML_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_RML_FML_FHD_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_RML_FML_FHD_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_RML_FML_FHD_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_RML_FML_TML_FEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_RML_FML_TML_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_RML_FML_TML_HEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HML_RML_FML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FML_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_FML_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_RML_FBL_FHD_TML_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_HML_RML_FBL_FHD_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_RML_FBL_FHD_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_RML_FBL_FHD_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_RML_FBL_TML_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_FBL_TML_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_RML_FBL_TML_HEB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_HML_RML_FBL_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_FBL_FEB_HEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HML_RML_FBL_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_RML_FHD_TML_FEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_HML_RML_FHD_TML_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_RML_FHD_TML_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FHD_FEB_TPB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_HML_RML_FHD_FEB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_HML_RML_FHD_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_RML_TML_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_HML_RML_TML_FEB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_RML_TML_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_HML_RML_FEB_TPB_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HML_FML_FBL_FHD_TML_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HML_FML_FBL_FHD_FEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HML_FML_FBL_FHD_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_FML_FBL_FHD_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_FML_FBL_TML_FEB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_HML_FML_FBL_TML_TPB_.dat\n",
      "0.8322440087145969\n",
      "model_lda_goldman_HML_FML_FBL_TML_HEB_.dat\n",
      "0.8104575163398693\n",
      "model_lda_goldman_HML_FML_FBL_FEB_TPB_.dat\n",
      "0.8300653594771242\n",
      "model_lda_goldman_HML_FML_FBL_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_FML_FBL_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FML_FHD_TML_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_FML_FHD_TML_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_FML_FHD_TML_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_FML_FHD_FEB_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_FML_FHD_FEB_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HML_FML_FHD_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_FML_TML_FEB_TPB_.dat\n",
      "0.8169934640522876\n",
      "model_lda_goldman_HML_FML_TML_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_FML_TML_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_FML_FEB_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_FBL_FHD_TML_FEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_FBL_FHD_TML_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_FBL_FHD_TML_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_FBL_TML_FEB_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_FBL_TML_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_FBL_TML_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_HML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_FHD_TML_FEB_TPB_.dat\n",
      "0.8736383442265795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_HML_FHD_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_FHD_TML_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_TML_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_RML_FML_FBL_TML_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_FML_FBL_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HHD_RML_FML_FBL_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_RML_FML_FHD_TML_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FML_FHD_FEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HHD_RML_FML_FHD_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_RML_FML_FHD_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FML_TML_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_FML_TML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_FML_TML_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_RML_FML_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HHD_RML_FML_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_RML_FML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_TML_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_RML_FBL_TML_FEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FBL_TML_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_RML_FBL_TML_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HHD_RML_FBL_FEB_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FBL_FEB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HHD_RML_FBL_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_RML_FHD_TML_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HHD_RML_FHD_TML_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FHD_TML_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_RML_FHD_FEB_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HHD_RML_FHD_FEB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_RML_FHD_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_RML_TML_FEB_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_RML_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_RML_TML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_RML_FEB_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_TML_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_FML_FBL_TML_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HHD_FML_FBL_TML_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_FML_FBL_TML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HHD_FML_FBL_FEB_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_FML_FBL_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_FML_FBL_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_FML_FHD_TML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_FML_FHD_TML_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_FML_FHD_TML_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_FML_FHD_FEB_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_FML_FHD_FEB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HHD_FML_FHD_TPB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_FML_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_FML_TML_FEB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HHD_FML_TML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_FML_FEB_TPB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_FBL_FHD_TML_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_FBL_FHD_TML_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_FBL_FHD_TML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_FBL_FHD_FEB_TPB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_HHD_FBL_FHD_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_FBL_FHD_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_FBL_TML_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HHD_FBL_TML_FEB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_FBL_TML_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_FBL_FEB_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_FHD_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_FHD_TML_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_FHD_TML_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HHD_FHD_FEB_TPB_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_RML_FML_FBL_FHD_TML_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_RML_FML_FBL_FHD_FEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_RML_FML_FBL_FHD_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_RML_FML_FBL_FHD_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_RML_FML_FBL_TML_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_RML_FML_FBL_TML_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FML_FBL_TML_HEB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_RML_FML_FBL_FEB_TPB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_RML_FML_FBL_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_FML_FBL_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FML_FHD_TML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_FML_FHD_TML_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_RML_FML_FHD_TML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_RML_FML_FHD_FEB_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_RML_FML_FHD_FEB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_RML_FML_FHD_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FML_TML_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_RML_FML_TML_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_RML_FML_TML_TPB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_RML_FML_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_RML_FBL_FHD_TML_FEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_RML_FBL_FHD_TML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_RML_FBL_FHD_TML_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_RML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_RML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_RML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_RML_FBL_TML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_RML_FBL_TML_FEB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_RML_FBL_TML_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_RML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FHD_TML_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_RML_FHD_TML_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_RML_FHD_TML_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_RML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_RML_TML_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_.dat\n",
      "0.9084967320261438\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_TML_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_TPB_.dat\n",
      "0.9084967320261438\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_TML_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FEB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_HEB_.dat\n",
      "0.8649237472766884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_TML_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_FEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_TPB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_TML_FEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_RML_TML_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_RML_TML_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FEB_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_RML_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_TML_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_TML_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_FML_TML_FEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_FML_TML_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_FML_TML_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FEB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_HHD_FML_TPB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_TML_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_FEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_TML_FEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_TML_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_TML_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_TML_FEB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_TML_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_TML_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_HHD_TML_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_TML_FEB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_TML_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_HHD_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_TML_.dat\n",
      "0.8061002178649237\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_TML_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_FEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_TPB_.dat\n",
      "0.9150326797385621\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_RML_FML_TML_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_RML_FML_TML_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_BIB_HML_RML_FML_TML_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_BIB_HML_RML_FML_FEB_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_RML_FML_FEB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_HML_RML_FML_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_FEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_HEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_BIB_HML_RML_FBL_TML_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_RML_FBL_TML_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_RML_FBL_TML_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FEB_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_RML_FBL_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HML_RML_FHD_TML_FEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_RML_FHD_TML_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_RML_FHD_TML_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_RML_FHD_FEB_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_RML_FHD_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_RML_FHD_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_RML_TML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_RML_TML_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_RML_TML_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HML_RML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_TML_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_FEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_FML_FBL_TML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_FML_FBL_TML_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_HML_FML_FBL_TML_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FEB_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FEB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_FML_FBL_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_FML_FHD_TML_FEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HML_FML_FHD_TML_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HML_FML_FHD_TML_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_FML_FHD_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_FML_FHD_FEB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_FML_FHD_TPB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_FML_TML_FEB_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_FML_TML_FEB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_FML_TML_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_FML_FEB_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_TML_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_TML_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_TML_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_FBL_TML_FEB_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_FBL_TML_FEB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_FBL_TML_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_FHD_TML_FEB_TPB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_FHD_TML_FEB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_FHD_TML_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_TML_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_TML_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_TML_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_RML_FML_TML_FEB_.dat\n",
      "0.8801742919389978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_BIB_HHD_RML_FML_TML_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_RML_FML_TML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FEB_TPB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_RML_FML_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_TML_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_FEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_TML_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_TML_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_TML_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FEB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_TML_FEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_TML_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_TML_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_TPB_HEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HHD_RML_TML_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_RML_TML_FEB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_RML_TML_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HHD_RML_FEB_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_TML_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_FEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_TML_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_TML_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_TML_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FEB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_TML_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_TML_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_TML_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HHD_FML_TML_FEB_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_FML_TML_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_FML_TML_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_FML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_TML_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_TML_TPB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_TML_HEB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HHD_FBL_TML_FEB_TPB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HHD_FBL_TML_FEB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_FBL_TML_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_FBL_FEB_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HHD_FHD_TML_FEB_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HHD_FHD_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_FHD_TML_TPB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_FHD_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_TML_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_RML_FML_FBL_TML_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_RML_FML_FBL_TML_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_RML_FML_FBL_TML_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FEB_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_RML_FML_FBL_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_RML_FML_FHD_TML_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_RML_FML_FHD_TML_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_RML_FML_FHD_TML_HEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_BIB_RML_FML_FHD_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_RML_FML_FHD_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_RML_FML_FHD_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_RML_FML_TML_FEB_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_RML_FML_TML_FEB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_RML_FML_TML_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_BIB_RML_FML_FEB_TPB_HEB_.dat\n",
      "0.906318082788671\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_TML_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_TML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_TML_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_TPB_HEB_.dat\n",
      "0.9106753812636166\n",
      "model_lda_goldman_BIB_RML_FBL_TML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_RML_FBL_TML_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_RML_FBL_TML_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_RML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_RML_FHD_TML_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_RML_FHD_TML_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_RML_FHD_TML_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_RML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_RML_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_TML_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_FEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_RML_FML_TML_FEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_HML_HHD_RML_FML_TML_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_HHD_RML_FML_TML_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_RML_FML_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_HML_HHD_RML_FML_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_RML_FML_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_RML_FBL_TML_FEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_RML_FBL_TML_TPB_.dat\n",
      "0.8605664488017429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_HML_HHD_RML_FBL_TML_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FEB_TPB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_HHD_RML_FBL_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_RML_FHD_TML_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HML_HHD_RML_FHD_TML_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_RML_FHD_TML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_RML_FHD_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_HHD_RML_FHD_FEB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_RML_FHD_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_HHD_RML_TML_FEB_TPB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_HML_HHD_RML_TML_FEB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_RML_TML_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HML_HHD_RML_FEB_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_TML_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_FEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_HHD_FML_FBL_TML_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_FML_FBL_TML_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_FML_FBL_TML_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_FML_FBL_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_FML_FHD_TML_FEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_FML_FHD_TML_TPB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_HML_HHD_FML_FHD_TML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_FML_FHD_FEB_TPB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_HML_HHD_FML_FHD_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_FML_FHD_TPB_HEB_.dat\n",
      "0.8235294117647058\n",
      "model_lda_goldman_HML_HHD_FML_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_FML_TML_FEB_HEB_.dat\n",
      "0.9041394335511983\n",
      "model_lda_goldman_HML_HHD_FML_TML_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_FML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_TML_FEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_TML_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_TML_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_FBL_TML_FEB_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_FBL_TML_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_FBL_TML_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_HHD_FBL_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_HHD_FHD_TML_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_FHD_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_HHD_FHD_TML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_HHD_FHD_FEB_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8387799564270153\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_TML_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_FEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_RML_FML_FBL_TML_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_FML_FBL_TML_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_RML_FML_FBL_TML_HEB_.dat\n",
      "0.8257080610021786\n",
      "model_lda_goldman_HML_RML_FML_FBL_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_RML_FML_FBL_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FML_FBL_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FML_FHD_TML_FEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_RML_FML_FHD_TML_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_RML_FML_FHD_TML_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_RML_FML_FHD_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_RML_FML_FHD_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_FML_FHD_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_RML_FML_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FML_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_RML_FML_TML_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_RML_FML_FEB_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FBL_FHD_TML_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_HML_RML_FBL_FHD_TML_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_RML_FBL_FHD_TML_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_RML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_RML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_RML_FBL_TML_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_RML_FBL_TML_FEB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_RML_FBL_TML_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_RML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_RML_FHD_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FHD_TML_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_RML_FHD_TML_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_HML_RML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_HML_RML_TML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_TML_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_FEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_HEB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_HHD_RML_FML_FBL_TML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_RML_FML_FBL_TML_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HHD_RML_FML_FBL_TML_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_FML_FBL_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HHD_RML_FML_FHD_TML_FEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FML_FHD_TML_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_RML_FML_FHD_TML_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_RML_FML_FHD_FEB_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_RML_FML_FHD_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HHD_RML_FML_FHD_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_RML_FML_TML_FEB_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HHD_RML_FML_TML_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HHD_RML_FML_TML_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_RML_FML_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_TML_FEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_TML_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_TML_HEB_.dat\n",
      "0.8453159041394336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_HHD_RML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_RML_FBL_TML_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_RML_FBL_TML_FEB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HHD_RML_FBL_TML_TPB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HHD_RML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_RML_FHD_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_RML_FHD_TML_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HHD_RML_FHD_TML_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HHD_RML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_RML_TML_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HHD_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HHD_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HHD_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HHD_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HHD_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HHD_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HHD_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_HHD_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HHD_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HHD_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_RML_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_RML_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_RML_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_RML_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_RML_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_RML_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_RML_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_RML_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_RML_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_RML_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_RML_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_RML_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_RML_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_RML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_RML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_RML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_RML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_TML_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_TML_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_FEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_TML_FEB_.dat\n",
      "0.906318082788671\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_TML_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_TML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FEB_TPB_.dat\n",
      "0.906318082788671\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FEB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_TML_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_TML_FEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_TML_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_TML_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FEB_TPB_.dat\n",
      "0.9041394335511983\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_TML_FEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_TML_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_TML_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_FEB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_HHD_RML_TML_FEB_TPB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_RML_TML_FEB_HEB_.dat\n",
      "0.9128540305010894\n",
      "model_lda_goldman_BIB_HML_HHD_RML_TML_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FEB_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_TML_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_TML_FEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_TML_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_TML_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FEB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_TML_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_TML_TPB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_TML_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_FEB_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_FML_TML_FEB_TPB_.dat\n",
      "0.9193899782135077\n",
      "model_lda_goldman_BIB_HML_HHD_FML_TML_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_FML_TML_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FEB_TPB_HEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_TML_FEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_TML_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_TML_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_FEB_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_TML_FEB_TPB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_TML_FEB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_TML_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_TML_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_TML_FEB_HEB_.dat\n",
      "0.8671023965141612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_BIB_HML_HHD_FHD_TML_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_FEB_TPB_HEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HML_HHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_TML_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_TML_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_TML_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_TML_HEB_.dat\n",
      "0.8344226579520697\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_TPB_HEB_.dat\n",
      "0.8431372549019608\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_TML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_TML_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_TML_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_RML_FML_TML_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_RML_FML_TML_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_RML_FML_TML_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_RML_FML_FEB_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_TML_FEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_TML_TPB_.dat\n",
      "0.9106753812636166\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_TML_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FBL_TML_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_RML_FBL_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_RML_FBL_TML_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_RML_FHD_TML_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_RML_FHD_TML_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_RML_FHD_TML_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_RML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_RML_TML_FEB_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HML_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.9193899782135077\n",
      "model_lda_goldman_BIB_HML_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_HML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_TML_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_FEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_TML_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_TML_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_TML_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FEB_TPB_.dat\n",
      "0.9041394335511983\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_TPB_HEB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_TML_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_TML_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_TML_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_FEB_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_RML_FML_TML_FEB_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FML_TML_FEB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_RML_FML_TML_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_TML_FEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_TML_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_TML_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_TML_FEB_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_TML_FEB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_TML_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_TML_FEB_TPB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_TML_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_TML_FEB_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.9128540305010894\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HHD_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.9106753812636166\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_RML_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_RML_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_RML_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8932461873638344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_BIB_RML_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_RML_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_RML_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_RML_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_RML_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_RML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_RML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_TML_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_FEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_TML_FEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_TML_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_TML_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FEB_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FEB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_TML_FEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_TML_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_TML_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_FEB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_HML_HHD_RML_FML_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_RML_FML_TML_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_RML_FML_TML_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_RML_FML_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_TML_FEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_TML_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_TML_HEB_.dat\n",
      "0.840958605664488\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_RML_FBL_TML_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_RML_FBL_TML_FEB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_HML_HHD_RML_FBL_TML_TPB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_RML_FHD_TML_FEB_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_RML_FHD_TML_FEB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_RML_FHD_TML_TPB_HEB_.dat\n",
      "0.8322440087145969\n",
      "model_lda_goldman_HML_HHD_RML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_RML_TML_FEB_TPB_HEB_.dat\n",
      "0.8453159041394336\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_HHD_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_HHD_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_HHD_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_HHD_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_RML_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_RML_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_RML_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_RML_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_RML_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_RML_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_RML_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HML_RML_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_RML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_HML_RML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_RML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_RML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_RML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HML_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_RML_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_RML_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_RML_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8366013071895425\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_HHD_RML_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_RML_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_RML_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HHD_RML_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_RML_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_HHD_RML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_RML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8605664488017429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_HHD_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8496732026143791\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_HHD_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HHD_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_RML_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_RML_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_RML_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_RML_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_RML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_TML_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_FEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_TML_FEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_TML_TPB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_TML_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FEB_TPB_.dat\n",
      "0.906318082788671\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FEB_HEB_.dat\n",
      "0.9106753812636166\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_TML_FEB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_TML_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_TML_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_FEB_TPB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_TML_FEB_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_TML_FEB_HEB_.dat\n",
      "0.906318082788671\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_TML_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_TML_FEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_TML_TPB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_TML_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_TML_FEB_TPB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_TML_FEB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_TML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_TML_FEB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_TML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_HHD_RML_TML_FEB_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.9041394335511983\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_RML_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.906318082788671\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.9150326797385621\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_RML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.9128540305010894\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_BIB_HHD_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HHD_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_RML_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_RML_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.9019607843137255\n",
      "model_lda_goldman_BIB_RML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_BIB_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_HML_HHD_RML_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_RML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_HML_HHD_RML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8518518518518519\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8474945533769063\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_HHD_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_HHD_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_RML_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_HML_RML_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8627450980392157\n",
      "model_lda_goldman_HML_RML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8605664488017429\n",
      "model_lda_goldman_HML_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HHD_RML_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HHD_RML_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HHD_RML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_HHD_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_RML_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8540305010893247\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_TML_FEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_TML_TPB_.dat\n",
      "0.9041394335511983\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_TML_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_FEB_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_FEB_HEB_.dat\n",
      "0.9041394335511983\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_TML_FEB_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_TML_FEB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_TML_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_TML_FEB_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_TML_TPB_HEB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.869281045751634\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8562091503267973\n",
      "model_lda_goldman_BIB_HML_HHD_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_RML_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_HML_RML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8954248366013072\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8976034858387799\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8714596949891068\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_BIB_HHD_RML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HHD_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8867102396514162\n",
      "model_lda_goldman_BIB_RML_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8758169934640523\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.8779956427015251\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8649237472766884\n",
      "model_lda_goldman_HML_HHD_RML_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_HML_HHD_RML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8583877995642701\n",
      "model_lda_goldman_HML_HHD_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_HML_RML_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_HHD_RML_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8910675381263616\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_TML_FEB_TPB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_TML_FEB_HEB_.dat\n",
      "0.8888888888888888\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_TML_TPB_HEB_.dat\n",
      "0.8845315904139434\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_FEB_TPB_HEB_.dat\n",
      "0.906318082788671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_TML_FEB_TPB_HEB_.dat\n",
      "0.8932461873638344\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8736383442265795\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8801742919389978\n",
      "model_lda_goldman_BIB_HML_HHD_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8671023965141612\n",
      "model_lda_goldman_BIB_HML_RML_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HHD_RML_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8997821350762527\n",
      "model_lda_goldman_HML_HHD_RML_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8823529411764706\n",
      "model_lda_goldman_BIB_HML_HHD_RML_FML_FBL_FHD_TML_FEB_TPB_HEB_.dat\n",
      "0.8910675381263616\n",
      "326\n",
      "2047\n"
     ]
    }
   ],
   "source": [
    "# Missing data models\n",
    "\n",
    "count_low_percentage = 0\n",
    "no_models = 0\n",
    "\n",
    "for k in range(1,12):\n",
    "    for combo in combinations(cols, k):  # 2 for pairs, 3 for triplets, etc\n",
    "        # take all combinations of certain length\n",
    "        col_list = list(combo)\n",
    "        # append the Sex column\n",
    "        col_list.append(\"Sex\")\n",
    "    \n",
    "        #Create DataFrames with the selected columns\n",
    "        df = iter_imputed_data_goldman.filter(col_list)\n",
    "    \n",
    "        # Shuffle the dat\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "        # Create feature matrix and target vector\n",
    "        X = df.drop('Sex', axis = 1).values\n",
    "        y = df['Sex']\n",
    "        \n",
    "        #print(len(y[y==1]))\n",
    "        \n",
    "        #split the dataset\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size=0.3, stratify=y)\n",
    "        \n",
    "        clf =  LinearDiscriminantAnalysis()\n",
    "        #clf  = GridSearchCV(classifier, parameters, cv=20)\n",
    "    \n",
    "        #with parallel_backend('threading', n_jobs=-1):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        \n",
    "        \n",
    "        if(score < 0.85):\n",
    "            count_low_percentage = count_low_percentage + 1\n",
    "            \n",
    "        no_models = no_models+1\n",
    "            \n",
    "            \n",
    "            \n",
    "        filename = \"model_lda_goldman_\"+\"_\".join(combo)+\"_.dat\"\n",
    "         \n",
    "        accuracy_filename = \"model_lda_goldman_\"+\"_\".join(combo)+\"_.txt\"\n",
    "        \n",
    "        with open(accuracy_filename, \"w\") as text_file:\n",
    "            text_file.write(str(score))\n",
    "                 \n",
    "        print(filename)\n",
    "        print(score)\n",
    "        \n",
    "        classifier = LinearDiscriminantAnalysis()\n",
    "        classifier.fit(X,y)\n",
    "        \n",
    "        pickle.dump(classifier, open(filename, \"wb\"))\n",
    "\n",
    "print(count_low_percentage)\n",
    "print(no_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
