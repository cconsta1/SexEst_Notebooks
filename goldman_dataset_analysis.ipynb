{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "\n",
    "# system\n",
    "\n",
    "import os\n",
    "\n",
    "# data analysis and plotting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import shapiro\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from xgboost import plot_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# data processing and model validation\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, explained_variance_score, confusion_matrix, accuracy_score, classification_report, log_loss\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "# classification libraries\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel, Matern, RationalQuadratic\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Importing imputation libs. \n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Hyperparameter optimization\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# web stuff\n",
    "import pickle\n",
    "\n",
    "# Various parameter settings\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# To install sklearn type \"pip install numpy scipy scikit-learn\" to the anaconda terminal\n",
    "\n",
    "# To change scientific numbers to float\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "\n",
    "# Increases the size of sns plots\n",
    "sns.set(rc={'figure.figsize':(12,10)})\n",
    "\n",
    "# import sys\n",
    "# !conda list Check the packages installed\n",
    "\n",
    "# Displaying all the rows/columns in a data set (the default option is not to show them)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data and create the datasets needed in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the raw data\n",
    "\n",
    "raw_data_goldman = pd.read_csv(\"datasets/Goldman.csv\", header = 0, encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Element:</th>\n",
       "      <th>LHUM</th>\n",
       "      <th>RHUM</th>\n",
       "      <th>LRAD</th>\n",
       "      <th>RRAD</th>\n",
       "      <th>LFEM</th>\n",
       "      <th>RFEM</th>\n",
       "      <th>LTIB</th>\n",
       "      <th>RTIB</th>\n",
       "      <th>OSCX</th>\n",
       "      <th>Metrics:</th>\n",
       "      <th>LHML</th>\n",
       "      <th>LHEB</th>\n",
       "      <th>LHHD</th>\n",
       "      <th>LHMLD</th>\n",
       "      <th>LHAPD</th>\n",
       "      <th>RHML</th>\n",
       "      <th>RHEB</th>\n",
       "      <th>RHHD</th>\n",
       "      <th>RHMLD</th>\n",
       "      <th>RHAPD</th>\n",
       "      <th>LRML</th>\n",
       "      <th>LRMLD</th>\n",
       "      <th>LRAPD</th>\n",
       "      <th>RRML</th>\n",
       "      <th>RRMLD</th>\n",
       "      <th>RRAPD</th>\n",
       "      <th>LFML</th>\n",
       "      <th>LFBL</th>\n",
       "      <th>LFEB</th>\n",
       "      <th>LFAB</th>\n",
       "      <th>LFHD</th>\n",
       "      <th>LFMLD</th>\n",
       "      <th>LFAPD</th>\n",
       "      <th>RFML</th>\n",
       "      <th>RFBL</th>\n",
       "      <th>RFEB</th>\n",
       "      <th>RFAB</th>\n",
       "      <th>RFHD</th>\n",
       "      <th>RFMLD</th>\n",
       "      <th>RFAPD</th>\n",
       "      <th>LTML</th>\n",
       "      <th>LTPB</th>\n",
       "      <th>LTMLD</th>\n",
       "      <th>LTAPD</th>\n",
       "      <th>RTML</th>\n",
       "      <th>RTPB</th>\n",
       "      <th>RTMLD</th>\n",
       "      <th>RTAPD</th>\n",
       "      <th>BIB</th>\n",
       "      <th>LIBL</th>\n",
       "      <th>RIBL</th>\n",
       "      <th>LAcH</th>\n",
       "      <th>RAcH</th>\n",
       "      <th>Derived:</th>\n",
       "      <th>Brachial</th>\n",
       "      <th>Crural</th>\n",
       "      <th>IL UL/LL</th>\n",
       "      <th>IL LL/UL</th>\n",
       "      <th>CBR FHD</th>\n",
       "      <th>McH FHD</th>\n",
       "      <th>GRINE FHD</th>\n",
       "      <th>AVG FHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1376.000000</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>1368.000000</td>\n",
       "      <td>1376.000000</td>\n",
       "      <td>1376.000000</td>\n",
       "      <td>1403.000000</td>\n",
       "      <td>1384.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1403.000000</td>\n",
       "      <td>1402.000000</td>\n",
       "      <td>1321.000000</td>\n",
       "      <td>1321.000000</td>\n",
       "      <td>1321.000000</td>\n",
       "      <td>1337.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1421.000000</td>\n",
       "      <td>1416.000000</td>\n",
       "      <td>1380.000000</td>\n",
       "      <td>1378.000000</td>\n",
       "      <td>1421.000000</td>\n",
       "      <td>1423.000000</td>\n",
       "      <td>1423.000000</td>\n",
       "      <td>1426.000000</td>\n",
       "      <td>1423.000000</td>\n",
       "      <td>1386.000000</td>\n",
       "      <td>1390.000000</td>\n",
       "      <td>1435.000000</td>\n",
       "      <td>1424.000000</td>\n",
       "      <td>1424.000000</td>\n",
       "      <td>1403.000000</td>\n",
       "      <td>1352.000000</td>\n",
       "      <td>1399.000000</td>\n",
       "      <td>1398.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1349.000000</td>\n",
       "      <td>1395.000000</td>\n",
       "      <td>1394.000000</td>\n",
       "      <td>1469.000000</td>\n",
       "      <td>1179.000000</td>\n",
       "      <td>1177.000000</td>\n",
       "      <td>1371.000000</td>\n",
       "      <td>1375.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1463.000000</td>\n",
       "      <td>1490.000000</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>1519.000000</td>\n",
       "      <td>1519.000000</td>\n",
       "      <td>1519.000000</td>\n",
       "      <td>1519.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100780</td>\n",
       "      <td>0.081274</td>\n",
       "      <td>0.140442</td>\n",
       "      <td>0.126788</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>0.064369</td>\n",
       "      <td>0.086476</td>\n",
       "      <td>0.086476</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>303.759811</td>\n",
       "      <td>57.442770</td>\n",
       "      <td>42.741615</td>\n",
       "      <td>19.884404</td>\n",
       "      <td>19.717754</td>\n",
       "      <td>307.556379</td>\n",
       "      <td>58.194581</td>\n",
       "      <td>43.003059</td>\n",
       "      <td>20.362117</td>\n",
       "      <td>20.462511</td>\n",
       "      <td>233.068887</td>\n",
       "      <td>14.097017</td>\n",
       "      <td>11.188312</td>\n",
       "      <td>234.964099</td>\n",
       "      <td>14.493866</td>\n",
       "      <td>11.335993</td>\n",
       "      <td>427.106967</td>\n",
       "      <td>423.455508</td>\n",
       "      <td>76.067754</td>\n",
       "      <td>66.361168</td>\n",
       "      <td>43.430837</td>\n",
       "      <td>25.767442</td>\n",
       "      <td>27.171792</td>\n",
       "      <td>425.657433</td>\n",
       "      <td>421.620169</td>\n",
       "      <td>76.259019</td>\n",
       "      <td>66.280950</td>\n",
       "      <td>43.497401</td>\n",
       "      <td>25.440857</td>\n",
       "      <td>27.243588</td>\n",
       "      <td>353.078403</td>\n",
       "      <td>69.344305</td>\n",
       "      <td>21.457984</td>\n",
       "      <td>26.351710</td>\n",
       "      <td>352.419429</td>\n",
       "      <td>69.343217</td>\n",
       "      <td>21.985778</td>\n",
       "      <td>25.372317</td>\n",
       "      <td>262.395848</td>\n",
       "      <td>150.980068</td>\n",
       "      <td>151.104503</td>\n",
       "      <td>48.873508</td>\n",
       "      <td>48.923360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765279</td>\n",
       "      <td>0.827734</td>\n",
       "      <td>0.692225</td>\n",
       "      <td>1.445639</td>\n",
       "      <td>60.145433</td>\n",
       "      <td>57.412799</td>\n",
       "      <td>62.073215</td>\n",
       "      <td>59.877149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301135</td>\n",
       "      <td>0.273345</td>\n",
       "      <td>0.347558</td>\n",
       "      <td>0.332844</td>\n",
       "      <td>0.255602</td>\n",
       "      <td>0.245489</td>\n",
       "      <td>0.281157</td>\n",
       "      <td>0.281157</td>\n",
       "      <td>0.140580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.025881</td>\n",
       "      <td>5.446128</td>\n",
       "      <td>4.134822</td>\n",
       "      <td>2.472297</td>\n",
       "      <td>2.345870</td>\n",
       "      <td>23.116218</td>\n",
       "      <td>5.544286</td>\n",
       "      <td>4.257656</td>\n",
       "      <td>2.573130</td>\n",
       "      <td>2.381074</td>\n",
       "      <td>18.985815</td>\n",
       "      <td>1.898293</td>\n",
       "      <td>1.321821</td>\n",
       "      <td>18.904538</td>\n",
       "      <td>1.938423</td>\n",
       "      <td>1.364567</td>\n",
       "      <td>31.509788</td>\n",
       "      <td>31.597694</td>\n",
       "      <td>6.204072</td>\n",
       "      <td>5.857932</td>\n",
       "      <td>3.972412</td>\n",
       "      <td>2.745311</td>\n",
       "      <td>2.874594</td>\n",
       "      <td>31.751905</td>\n",
       "      <td>31.843808</td>\n",
       "      <td>6.238232</td>\n",
       "      <td>5.832092</td>\n",
       "      <td>4.053790</td>\n",
       "      <td>2.663129</td>\n",
       "      <td>2.915361</td>\n",
       "      <td>28.076348</td>\n",
       "      <td>5.773576</td>\n",
       "      <td>2.376611</td>\n",
       "      <td>3.022335</td>\n",
       "      <td>28.749863</td>\n",
       "      <td>5.775864</td>\n",
       "      <td>2.468692</td>\n",
       "      <td>2.783156</td>\n",
       "      <td>18.274559</td>\n",
       "      <td>10.573547</td>\n",
       "      <td>10.753604</td>\n",
       "      <td>4.052208</td>\n",
       "      <td>4.079195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>0.027455</td>\n",
       "      <td>0.018438</td>\n",
       "      <td>0.038447</td>\n",
       "      <td>7.946894</td>\n",
       "      <td>8.914306</td>\n",
       "      <td>9.029766</td>\n",
       "      <td>8.555195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.500000</td>\n",
       "      <td>36.510000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>12.010000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>32.240000</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>13.840000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>8.740000</td>\n",
       "      <td>7.370000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>9.160000</td>\n",
       "      <td>7.880000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>309.500000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>49.990000</td>\n",
       "      <td>33.640000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>18.880000</td>\n",
       "      <td>341.500000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>49.830000</td>\n",
       "      <td>32.970000</td>\n",
       "      <td>17.970000</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>15.220000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>18.590000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>38.640000</td>\n",
       "      <td>37.890000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677912</td>\n",
       "      <td>0.692025</td>\n",
       "      <td>0.599260</td>\n",
       "      <td>1.299290</td>\n",
       "      <td>36.092754</td>\n",
       "      <td>34.692285</td>\n",
       "      <td>39.058420</td>\n",
       "      <td>38.300225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>287.375000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>39.760000</td>\n",
       "      <td>18.230000</td>\n",
       "      <td>18.030000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>39.900000</td>\n",
       "      <td>18.690000</td>\n",
       "      <td>18.732500</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>12.780000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>13.117500</td>\n",
       "      <td>10.327500</td>\n",
       "      <td>404.500000</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>62.010000</td>\n",
       "      <td>40.610000</td>\n",
       "      <td>23.830000</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>61.970000</td>\n",
       "      <td>40.510000</td>\n",
       "      <td>23.535000</td>\n",
       "      <td>25.097500</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>19.780000</td>\n",
       "      <td>24.180000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>20.245000</td>\n",
       "      <td>23.380000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>45.995000</td>\n",
       "      <td>46.055000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.744615</td>\n",
       "      <td>0.809212</td>\n",
       "      <td>0.680304</td>\n",
       "      <td>1.421166</td>\n",
       "      <td>54.614902</td>\n",
       "      <td>50.992205</td>\n",
       "      <td>55.569460</td>\n",
       "      <td>53.738995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>303.500000</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>42.715000</td>\n",
       "      <td>19.855000</td>\n",
       "      <td>19.640000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>43.015000</td>\n",
       "      <td>20.300000</td>\n",
       "      <td>20.460000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>14.050000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>11.270000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>66.615000</td>\n",
       "      <td>43.540000</td>\n",
       "      <td>25.730000</td>\n",
       "      <td>27.160000</td>\n",
       "      <td>426.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>66.190000</td>\n",
       "      <td>43.520000</td>\n",
       "      <td>25.370000</td>\n",
       "      <td>27.135000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>21.370000</td>\n",
       "      <td>26.235000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>21.890000</td>\n",
       "      <td>25.215000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>49.050000</td>\n",
       "      <td>49.030000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765331</td>\n",
       "      <td>0.828423</td>\n",
       "      <td>0.691577</td>\n",
       "      <td>1.445971</td>\n",
       "      <td>60.046353</td>\n",
       "      <td>57.474110</td>\n",
       "      <td>62.135320</td>\n",
       "      <td>59.832163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>45.722500</td>\n",
       "      <td>21.460000</td>\n",
       "      <td>21.300000</td>\n",
       "      <td>323.500000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>22.035000</td>\n",
       "      <td>22.187500</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>15.320000</td>\n",
       "      <td>12.120000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>15.742500</td>\n",
       "      <td>12.312500</td>\n",
       "      <td>448.500000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>70.822500</td>\n",
       "      <td>46.150000</td>\n",
       "      <td>27.650000</td>\n",
       "      <td>29.175000</td>\n",
       "      <td>447.500000</td>\n",
       "      <td>443.500000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>70.605000</td>\n",
       "      <td>46.395000</td>\n",
       "      <td>27.242500</td>\n",
       "      <td>29.285000</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>23.030000</td>\n",
       "      <td>28.537500</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>23.790000</td>\n",
       "      <td>27.317500</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>51.715000</td>\n",
       "      <td>51.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.785749</td>\n",
       "      <td>0.845936</td>\n",
       "      <td>0.703647</td>\n",
       "      <td>1.469932</td>\n",
       "      <td>65.196007</td>\n",
       "      <td>63.737713</td>\n",
       "      <td>68.480050</td>\n",
       "      <td>65.676422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>56.330000</td>\n",
       "      <td>27.220000</td>\n",
       "      <td>27.290000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>55.670000</td>\n",
       "      <td>30.440000</td>\n",
       "      <td>27.190000</td>\n",
       "      <td>290.500000</td>\n",
       "      <td>22.150000</td>\n",
       "      <td>16.280000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>23.220000</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>531.000000</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>93.500000</td>\n",
       "      <td>83.680000</td>\n",
       "      <td>57.390000</td>\n",
       "      <td>34.540000</td>\n",
       "      <td>37.060000</td>\n",
       "      <td>532.500000</td>\n",
       "      <td>531.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>82.850000</td>\n",
       "      <td>57.860000</td>\n",
       "      <td>33.840000</td>\n",
       "      <td>37.530000</td>\n",
       "      <td>446.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>37.940000</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>29.830000</td>\n",
       "      <td>35.200000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>62.940000</td>\n",
       "      <td>62.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>0.965422</td>\n",
       "      <td>0.769651</td>\n",
       "      <td>1.668724</td>\n",
       "      <td>92.745113</td>\n",
       "      <td>89.122375</td>\n",
       "      <td>94.193500</td>\n",
       "      <td>92.020329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Element:         LHUM         RHUM         LRAD         RRAD  \\\n",
       "count       0.0  1538.000000  1538.000000  1538.000000  1538.000000   \n",
       "mean        NaN     0.100780     0.081274     0.140442     0.126788   \n",
       "std         NaN     0.301135     0.273345     0.347558     0.332844   \n",
       "min         NaN     0.000000     0.000000     0.000000     0.000000   \n",
       "25%         NaN     0.000000     0.000000     0.000000     0.000000   \n",
       "50%         NaN     0.000000     0.000000     0.000000     0.000000   \n",
       "75%         NaN     0.000000     0.000000     0.000000     0.000000   \n",
       "max         NaN     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              LFEM         RFEM         LTIB         RTIB         OSCX  \\\n",
       "count  1538.000000  1538.000000  1538.000000  1538.000000  1538.000000   \n",
       "mean      0.070221     0.064369     0.086476     0.086476     0.020156   \n",
       "std       0.255602     0.245489     0.281157     0.281157     0.140580   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       Metrics:         LHML         LHEB         LHHD        LHMLD  \\\n",
       "count       0.0  1376.000000  1354.000000  1368.000000  1376.000000   \n",
       "mean        NaN   303.759811    57.442770    42.741615    19.884404   \n",
       "std         NaN    23.025881     5.446128     4.134822     2.472297   \n",
       "min         NaN   229.500000    36.510000    29.580000    12.010000   \n",
       "25%         NaN   287.375000    53.000000    39.760000    18.230000   \n",
       "50%         NaN   303.500000    57.500000    42.715000    19.855000   \n",
       "75%         NaN   319.000000    61.000000    45.722500    21.460000   \n",
       "max         NaN   376.000000    75.000000    56.330000    27.220000   \n",
       "\n",
       "             LHAPD         RHML         RHEB         RHHD        RHMLD  \\\n",
       "count  1376.000000  1403.000000  1384.000000  1396.000000  1403.000000   \n",
       "mean     19.717754   307.556379    58.194581    43.003059    20.362117   \n",
       "std       2.345870    23.116218     5.544286     4.257656     2.573130   \n",
       "min      13.000000   234.000000    42.000000    32.240000    12.250000   \n",
       "25%      18.030000   291.000000    54.000000    39.900000    18.690000   \n",
       "50%      19.640000   307.000000    58.000000    43.015000    20.300000   \n",
       "75%      21.300000   323.500000    62.000000    46.000000    22.035000   \n",
       "max      27.290000   383.000000    75.000000    55.670000    30.440000   \n",
       "\n",
       "             RHAPD         LRML        LRMLD        LRAPD         RRML  \\\n",
       "count  1402.000000  1321.000000  1321.000000  1321.000000  1337.000000   \n",
       "mean     20.462511   233.068887    14.097017    11.188312   234.964099   \n",
       "std       2.381074    18.985815     1.898293     1.321821    18.904538   \n",
       "min      13.840000   179.000000     8.740000     7.370000   180.000000   \n",
       "25%      18.732500   219.000000    12.780000    10.230000   221.000000   \n",
       "50%      20.460000   233.000000    14.050000    11.110000   235.000000   \n",
       "75%      22.187500   247.000000    15.320000    12.120000   248.000000   \n",
       "max      27.190000   290.500000    22.150000    16.280000   291.000000   \n",
       "\n",
       "             RRMLD        RRAPD         LFML         LFBL         LFEB  \\\n",
       "count  1340.000000  1340.000000  1421.000000  1416.000000  1380.000000   \n",
       "mean     14.493866    11.335993   427.106967   423.455508    76.067754   \n",
       "std       1.938423     1.364567    31.509788    31.597694     6.204072   \n",
       "min       9.160000     7.880000   345.000000   309.500000    58.000000   \n",
       "25%      13.117500    10.327500   404.500000   401.000000    72.000000   \n",
       "50%      14.400000    11.270000   428.000000   424.000000    76.000000   \n",
       "75%      15.742500    12.312500   448.500000   445.000000    81.000000   \n",
       "max      23.220000    15.680000   531.000000   530.000000    93.500000   \n",
       "\n",
       "              LFAB         LFHD        LFMLD        LFAPD         RFML  \\\n",
       "count  1378.000000  1421.000000  1423.000000  1423.000000  1426.000000   \n",
       "mean     66.361168    43.430837    25.767442    27.171792   425.657433   \n",
       "std       5.857932     3.972412     2.745311     2.874594    31.751905   \n",
       "min      49.990000    33.640000    17.900000    18.880000   341.500000   \n",
       "25%      62.010000    40.610000    23.830000    25.120000   403.000000   \n",
       "50%      66.615000    43.540000    25.730000    27.160000   426.000000   \n",
       "75%      70.822500    46.150000    27.650000    29.175000   447.500000   \n",
       "max      83.680000    57.390000    34.540000    37.060000   532.500000   \n",
       "\n",
       "              RFBL         RFEB         RFAB         RFHD        RFMLD  \\\n",
       "count  1423.000000  1386.000000  1390.000000  1435.000000  1424.000000   \n",
       "mean    421.620169    76.259019    66.280950    43.497401    25.440857   \n",
       "std      31.843808     6.238232     5.832092     4.053790     2.663129   \n",
       "min     279.000000    58.000000    49.830000    32.970000    17.970000   \n",
       "25%     399.000000    72.000000    61.970000    40.510000    23.535000   \n",
       "50%     422.000000    76.000000    66.190000    43.520000    25.370000   \n",
       "75%     443.500000    81.000000    70.605000    46.395000    27.242500   \n",
       "max     531.000000    94.000000    82.850000    57.860000    33.840000   \n",
       "\n",
       "             RFAPD         LTML         LTPB        LTMLD        LTAPD  \\\n",
       "count  1424.000000  1403.000000  1352.000000  1399.000000  1398.000000   \n",
       "mean     27.243588   353.078403    69.344305    21.457984    26.351710   \n",
       "std       2.915361    28.076348     5.773576     2.376611     3.022335   \n",
       "min      18.340000   276.000000    52.000000    15.220000    19.000000   \n",
       "25%      25.097500   333.000000    65.000000    19.780000    24.180000   \n",
       "50%      27.135000   353.000000    69.500000    21.370000    26.235000   \n",
       "75%      29.285000   372.000000    74.000000    23.030000    28.537500   \n",
       "max      37.530000   446.000000    86.000000    31.000000    37.940000   \n",
       "\n",
       "              RTML         RTPB        RTMLD        RTAPD          BIB  \\\n",
       "count  1400.000000  1349.000000  1395.000000  1394.000000  1469.000000   \n",
       "mean    352.419429    69.343217    21.985778    25.372317   262.395848   \n",
       "std      28.749863     5.775864     2.468692     2.783156    18.274559   \n",
       "min     237.000000    50.000000    15.150000    18.590000   184.000000   \n",
       "25%     332.000000    65.000000    20.245000    23.380000   251.000000   \n",
       "50%     353.000000    69.000000    21.890000    25.215000   263.000000   \n",
       "75%     372.000000    74.000000    23.790000    27.317500   274.000000   \n",
       "max     444.000000    85.000000    29.830000    35.200000   324.000000   \n",
       "\n",
       "              LIBL         RIBL         LAcH         RAcH  Derived:  \\\n",
       "count  1179.000000  1177.000000  1371.000000  1375.000000       0.0   \n",
       "mean    150.980068   151.104503    48.873508    48.923360       NaN   \n",
       "std      10.573547    10.753604     4.052208     4.079195       NaN   \n",
       "min     105.000000   106.000000    38.640000    37.890000       NaN   \n",
       "25%     145.000000   145.000000    45.995000    46.055000       NaN   \n",
       "50%     151.000000   151.000000    49.050000    49.030000       NaN   \n",
       "75%     158.000000   158.000000    51.715000    51.800000       NaN   \n",
       "max     181.000000   189.000000    62.940000    62.990000       NaN   \n",
       "\n",
       "          Brachial       Crural     IL UL/LL     IL LL/UL      CBR FHD  \\\n",
       "count  1463.000000  1490.000000  1418.000000  1418.000000  1519.000000   \n",
       "mean      0.765279     0.827734     0.692225     1.445639    60.145433   \n",
       "std       0.031805     0.027455     0.018438     0.038447     7.946894   \n",
       "min       0.677912     0.692025     0.599260     1.299290    36.092754   \n",
       "25%       0.744615     0.809212     0.680304     1.421166    54.614902   \n",
       "50%       0.765331     0.828423     0.691577     1.445971    60.046353   \n",
       "75%       0.785749     0.845936     0.703647     1.469932    65.196007   \n",
       "max       1.076923     0.965422     0.769651     1.668724    92.745113   \n",
       "\n",
       "           McH FHD    GRINE FHD      AVG FHD  \n",
       "count  1519.000000  1519.000000  1519.000000  \n",
       "mean     57.412799    62.073215    59.877149  \n",
       "std       8.914306     9.029766     8.555195  \n",
       "min      34.692285    39.058420    38.300225  \n",
       "25%      50.992205    55.569460    53.738995  \n",
       "50%      57.474110    62.135320    59.832163  \n",
       "75%      63.737713    68.480050    65.676422  \n",
       "max      89.122375    94.193500    92.020329  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new data set that contains all the measured data, i.e.,\n",
    "# not derived data as well as the class label Sex for each measurement\n",
    "\n",
    "measured_data_goldman = raw_data_goldman.loc[:,\"LHML\":\"RAcH\"]\n",
    "\n",
    "# Fill missing data with zeroes so that we can average between left and right skeletal measurements below\n",
    "\n",
    "measured_data_goldman = measured_data_goldman.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               BIB          HML          HHD          RML          FML  \\\n",
      "count  1538.000000  1538.000000  1538.000000  1538.000000  1538.000000   \n",
      "mean    250.623862   298.197042    41.699971   225.650033   420.892230   \n",
      "std      57.194643    52.024020     8.025574    46.894808    57.592097   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%     249.000000   288.312500    39.520000   218.500000   402.750000   \n",
      "50%     262.000000   304.500000    42.700000   233.000000   426.375000   \n",
      "75%     274.000000   320.750000    45.728750   247.000000   447.937500   \n",
      "max     324.000000   379.500000    55.120000   290.250000   531.750000   \n",
      "\n",
      "               FBL          FHD          TML  \n",
      "count  1538.000000  1538.000000  1538.000000  \n",
      "mean    416.541125    42.925692   343.531632  \n",
      "std      59.136325     6.222401    63.412299  \n",
      "min       0.000000     0.000000     0.000000  \n",
      "25%     399.000000    40.446250   330.812500  \n",
      "50%     422.500000    43.432500   352.500000  \n",
      "75%     444.250000    46.258750   372.000000  \n",
      "max     530.500000    57.625000   444.500000  \n"
     ]
    }
   ],
   "source": [
    "# Create new columns and take the average between left and right skeletal measurements\n",
    "\n",
    "target_cols = ['HML', 'HHD', 'RML', 'FML', 'FBL','FHD', 'TML']\n",
    "\n",
    "\n",
    "for col in target_cols:\n",
    "    measured_data_goldman[col] = 0.\n",
    "    \n",
    "    min_left_col_value = measured_data_goldman[\"\".join([\"L\",col])][measured_data_goldman[\"\".join([\"L\",col])] > 0.1].min() - 1\n",
    "    \n",
    "    min_right_col_value = measured_data_goldman[\"\".join([\"R\",col])][measured_data_goldman[\"\".join([\"R\",col])] > 0.1].min() - 1\n",
    "    \n",
    "    measured_data_goldman.loc[(measured_data_goldman[\"\".join([\"L\",col])] < 0.1) & (measured_data_goldman[\"\".join([\"R\",col])] > min_right_col_value), \n",
    "        col] = measured_data_goldman[\"\".join([\"R\",col])]\n",
    "\n",
    "    measured_data_goldman.loc[(measured_data_goldman[\"\".join([\"R\",col])] < 0.1) & (measured_data_goldman[\"\".join([\"L\",col])] > min_left_col_value), \n",
    "       col] = measured_data_goldman[\"\".join([\"L\",col])]\n",
    "\n",
    "    measured_data_goldman.loc[(measured_data_goldman[\"\".join([\"R\",col])] > min_right_col_value) & (measured_data_goldman[\"\".join([\"L\",col])] > min_left_col_value), \n",
    "       col] = (measured_data_goldman[\"\".join([\"L\",col])] + measured_data_goldman[\"\".join([\"R\",col])])/2\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create a dataset with the features we will use to build our models\n",
    "\n",
    "model_cols = ['BIB','HML', 'HHD', 'RML', 'FML', 'FBL','FHD', 'TML']\n",
    "\n",
    "model_data_goldman = measured_data_goldman.drop(columns=[col for col in measured_data_goldman if col not in model_cols])\n",
    "\n",
    "# Add the Sex column\n",
    "\n",
    "model_data_goldman = pd.concat([model_data_goldman.loc[:,:],raw_data_goldman.loc[:,\"Sex\"]],axis=1)\n",
    "\n",
    "print(model_data_goldman.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>HML</th>\n",
       "      <th>HHD</th>\n",
       "      <th>RML</th>\n",
       "      <th>FML</th>\n",
       "      <th>FBL</th>\n",
       "      <th>FHD</th>\n",
       "      <th>TML</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>250.544830</td>\n",
       "      <td>298.170681</td>\n",
       "      <td>41.714529</td>\n",
       "      <td>225.744928</td>\n",
       "      <td>420.889234</td>\n",
       "      <td>416.533704</td>\n",
       "      <td>42.939784</td>\n",
       "      <td>343.450524</td>\n",
       "      <td>0.355366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57.361899</td>\n",
       "      <td>52.176096</td>\n",
       "      <td>8.047175</td>\n",
       "      <td>46.673734</td>\n",
       "      <td>57.745325</td>\n",
       "      <td>59.293408</td>\n",
       "      <td>6.237500</td>\n",
       "      <td>63.587332</td>\n",
       "      <td>0.478781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>249.000000</td>\n",
       "      <td>288.250000</td>\n",
       "      <td>39.530000</td>\n",
       "      <td>218.500000</td>\n",
       "      <td>402.562500</td>\n",
       "      <td>398.937500</td>\n",
       "      <td>40.470000</td>\n",
       "      <td>330.687500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>262.000000</td>\n",
       "      <td>304.500000</td>\n",
       "      <td>42.735000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>426.500000</td>\n",
       "      <td>422.625000</td>\n",
       "      <td>43.460000</td>\n",
       "      <td>352.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>320.750000</td>\n",
       "      <td>45.762500</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>447.812500</td>\n",
       "      <td>444.250000</td>\n",
       "      <td>46.277500</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>55.120000</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>531.750000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>57.625000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BIB          HML          HHD          RML          FML  \\\n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  1528.000000   \n",
       "mean    250.544830   298.170681    41.714529   225.744928   420.889234   \n",
       "std      57.361899    52.176096     8.047175    46.673734    57.745325   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%     249.000000   288.250000    39.530000   218.500000   402.562500   \n",
       "50%     262.000000   304.500000    42.735000   233.000000   426.500000   \n",
       "75%     274.000000   320.750000    45.762500   247.000000   447.812500   \n",
       "max     324.000000   379.500000    55.120000   290.250000   531.750000   \n",
       "\n",
       "               FBL          FHD          TML          Sex  \n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  \n",
       "mean    416.533704    42.939784   343.450524     0.355366  \n",
       "std      59.293408     6.237500    63.587332     0.478781  \n",
       "min       0.000000     0.000000     0.000000     0.000000  \n",
       "25%     398.937500    40.470000   330.687500     0.000000  \n",
       "50%     422.625000    43.460000   352.500000     0.000000  \n",
       "75%     444.250000    46.277500   372.000000     1.000000  \n",
       "max     530.500000    57.625000   444.500000     1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the Sex column is a string, not a value, that's why\n",
    "# its not printed above. \n",
    "\n",
    "# But we take advandage of the fact that its a string to \n",
    "# drop the values 1? and 0?\n",
    "\n",
    "# Get rid of 1? and 0? from sex estimation and then shuffle the dataset\n",
    "# because otherwise you have 1 and 0 packed together \n",
    "\n",
    "model_data_goldman = pd.concat([model_data_goldman.loc[model_data_goldman['Sex']=='1'], model_data_goldman.loc[model_data_goldman['Sex']=='0']])\n",
    "\n",
    "model_data_goldman = model_data_goldman.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Now convert Sex from string to int\n",
    "\n",
    "model_data_goldman[\"Sex\"] = model_data_goldman[\"Sex\"].astype(int) \n",
    "\n",
    "model_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 0.0 back to nan to better handle the dataset within xgboost but\n",
    "# also to become able to drop the NA entries easily\n",
    "\n",
    "model_data_goldman[model_cols] = model_data_goldman[model_cols].replace(0.0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>HML</th>\n",
       "      <th>HHD</th>\n",
       "      <th>RML</th>\n",
       "      <th>FML</th>\n",
       "      <th>FBL</th>\n",
       "      <th>FHD</th>\n",
       "      <th>TML</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1491.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>1508.00000</td>\n",
       "      <td>1506.000000</td>\n",
       "      <td>1509.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>262.393763</td>\n",
       "      <td>305.569953</td>\n",
       "      <td>42.864694</td>\n",
       "      <td>233.856441</td>\n",
       "      <td>426.47132</td>\n",
       "      <td>422.618526</td>\n",
       "      <td>43.480444</td>\n",
       "      <td>352.920242</td>\n",
       "      <td>0.355366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.299914</td>\n",
       "      <td>22.965932</td>\n",
       "      <td>4.148381</td>\n",
       "      <td>18.935423</td>\n",
       "      <td>31.56893</td>\n",
       "      <td>31.525125</td>\n",
       "      <td>3.984079</td>\n",
       "      <td>28.471105</td>\n",
       "      <td>0.478781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>179.500000</td>\n",
       "      <td>343.75000</td>\n",
       "      <td>329.750000</td>\n",
       "      <td>33.315000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>251.000000</td>\n",
       "      <td>289.500000</td>\n",
       "      <td>39.805000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>404.00000</td>\n",
       "      <td>399.812500</td>\n",
       "      <td>40.635000</td>\n",
       "      <td>332.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>263.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>42.920000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>427.00000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>43.505000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>321.250000</td>\n",
       "      <td>45.810000</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>448.00000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>46.305000</td>\n",
       "      <td>372.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>55.120000</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>531.75000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>57.625000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BIB          HML          HHD          RML         FML  \\\n",
       "count  1459.000000  1491.000000  1487.000000  1475.000000  1508.00000   \n",
       "mean    262.393763   305.569953    42.864694   233.856441   426.47132   \n",
       "std      18.299914    22.965932     4.148381    18.935423    31.56893   \n",
       "min     184.000000   234.000000    29.580000   179.500000   343.75000   \n",
       "25%     251.000000   289.500000    39.805000   220.000000   404.00000   \n",
       "50%     263.000000   305.000000    42.920000   234.000000   427.00000   \n",
       "75%     274.000000   321.250000    45.810000   247.500000   448.00000   \n",
       "max     324.000000   379.500000    55.120000   290.250000   531.75000   \n",
       "\n",
       "               FBL          FHD          TML          Sex  \n",
       "count  1506.000000  1509.000000  1487.000000  1528.000000  \n",
       "mean    422.618526    43.480444   352.920242     0.355366  \n",
       "std      31.525125     3.984079    28.471105     0.478781  \n",
       "min     329.750000    33.315000   271.000000     0.000000  \n",
       "25%     399.812500    40.635000   332.500000     0.000000  \n",
       "50%     423.000000    43.505000   353.000000     0.000000  \n",
       "75%     444.500000    46.305000   372.250000     1.000000  \n",
       "max     530.500000    57.625000   444.500000     1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_zeroes_model_data_goldman = model_data_goldman.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>HML</th>\n",
       "      <th>HHD</th>\n",
       "      <th>RML</th>\n",
       "      <th>FML</th>\n",
       "      <th>FBL</th>\n",
       "      <th>FHD</th>\n",
       "      <th>TML</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1338.000000</td>\n",
       "      <td>1338.000000</td>\n",
       "      <td>1338.00000</td>\n",
       "      <td>1338.000000</td>\n",
       "      <td>1338.000000</td>\n",
       "      <td>1338.000000</td>\n",
       "      <td>1338.000000</td>\n",
       "      <td>1338.000000</td>\n",
       "      <td>1338.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>262.366592</td>\n",
       "      <td>306.079821</td>\n",
       "      <td>42.90102</td>\n",
       "      <td>233.922459</td>\n",
       "      <td>427.145740</td>\n",
       "      <td>423.257100</td>\n",
       "      <td>43.533487</td>\n",
       "      <td>353.314387</td>\n",
       "      <td>0.355755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.255714</td>\n",
       "      <td>22.667387</td>\n",
       "      <td>4.08016</td>\n",
       "      <td>18.783494</td>\n",
       "      <td>31.185288</td>\n",
       "      <td>31.154284</td>\n",
       "      <td>3.947922</td>\n",
       "      <td>28.194530</td>\n",
       "      <td>0.478920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>31.65000</td>\n",
       "      <td>179.500000</td>\n",
       "      <td>343.750000</td>\n",
       "      <td>329.750000</td>\n",
       "      <td>33.315000</td>\n",
       "      <td>276.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>251.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>39.87875</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>401.312500</td>\n",
       "      <td>40.735000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>263.000000</td>\n",
       "      <td>305.250000</td>\n",
       "      <td>42.92750</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>423.250000</td>\n",
       "      <td>43.522500</td>\n",
       "      <td>353.375000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>321.750000</td>\n",
       "      <td>45.79750</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>444.750000</td>\n",
       "      <td>46.343750</td>\n",
       "      <td>372.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>54.76000</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>531.750000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>56.195000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BIB          HML         HHD          RML          FML  \\\n",
       "count  1338.000000  1338.000000  1338.00000  1338.000000  1338.000000   \n",
       "mean    262.366592   306.079821    42.90102   233.922459   427.145740   \n",
       "std      18.255714    22.667387     4.08016    18.783494    31.185288   \n",
       "min     184.000000   234.000000    31.65000   179.500000   343.750000   \n",
       "25%     251.000000   290.000000    39.87875   220.500000   405.000000   \n",
       "50%     263.000000   305.250000    42.92750   234.000000   427.000000   \n",
       "75%     274.000000   321.750000    45.79750   247.500000   448.000000   \n",
       "max     324.000000   379.500000    54.76000   290.250000   531.750000   \n",
       "\n",
       "               FBL          FHD          TML          Sex  \n",
       "count  1338.000000  1338.000000  1338.000000  1338.000000  \n",
       "mean    423.257100    43.533487   353.314387     0.355755  \n",
       "std      31.154284     3.947922    28.194530     0.478920  \n",
       "min     329.750000    33.315000   276.500000     0.000000  \n",
       "25%     401.312500    40.735000   333.000000     0.000000  \n",
       "50%     423.250000    43.522500   353.375000     0.000000  \n",
       "75%     444.750000    46.343750   372.250000     1.000000  \n",
       "max     530.500000    56.195000   444.500000     1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_zeroes_model_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data set using the knn imputer\n",
    "# Here we use the 3 nearest neighbors to calculate the missing data\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3, missing_values=0.0)\n",
    "\n",
    "sex_column = model_data_goldman['Sex']\n",
    "\n",
    "temporary_data_set = model_data_goldman.fillna(0.).drop([\"Sex\"],axis=1)\n",
    "\n",
    "cols = temporary_data_set.columns\n",
    "\n",
    "temporary_data_set = knn_imputer.fit_transform(temporary_data_set)\n",
    "\n",
    "temporary_data_set = pd.DataFrame(data=temporary_data_set, columns=cols)\n",
    "\n",
    "knn_imputed_data_goldman = pd.concat([temporary_data_set,sex_column],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>HML</th>\n",
       "      <th>HHD</th>\n",
       "      <th>RML</th>\n",
       "      <th>FML</th>\n",
       "      <th>FBL</th>\n",
       "      <th>FHD</th>\n",
       "      <th>TML</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>262.323953</td>\n",
       "      <td>305.530574</td>\n",
       "      <td>42.863991</td>\n",
       "      <td>233.829407</td>\n",
       "      <td>426.414921</td>\n",
       "      <td>422.533213</td>\n",
       "      <td>43.476951</td>\n",
       "      <td>352.984337</td>\n",
       "      <td>0.355366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.096663</td>\n",
       "      <td>22.855956</td>\n",
       "      <td>4.113545</td>\n",
       "      <td>18.824000</td>\n",
       "      <td>31.476768</td>\n",
       "      <td>31.435141</td>\n",
       "      <td>3.969791</td>\n",
       "      <td>28.390340</td>\n",
       "      <td>0.478781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>179.500000</td>\n",
       "      <td>343.750000</td>\n",
       "      <td>329.750000</td>\n",
       "      <td>33.315000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>251.000000</td>\n",
       "      <td>289.500000</td>\n",
       "      <td>39.820000</td>\n",
       "      <td>220.437500</td>\n",
       "      <td>404.187500</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>40.638750</td>\n",
       "      <td>332.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>263.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>42.925000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>426.750000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>43.502500</td>\n",
       "      <td>353.125000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>321.250000</td>\n",
       "      <td>45.781250</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>46.286250</td>\n",
       "      <td>372.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>55.120000</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>531.750000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>57.625000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BIB          HML          HHD          RML          FML  \\\n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  1528.000000   \n",
       "mean    262.323953   305.530574    42.863991   233.829407   426.414921   \n",
       "std      18.096663    22.855956     4.113545    18.824000    31.476768   \n",
       "min     184.000000   234.000000    29.580000   179.500000   343.750000   \n",
       "25%     251.000000   289.500000    39.820000   220.437500   404.187500   \n",
       "50%     263.000000   305.000000    42.925000   234.000000   426.750000   \n",
       "75%     274.000000   321.250000    45.781250   247.500000   448.000000   \n",
       "max     324.000000   379.500000    55.120000   290.250000   531.750000   \n",
       "\n",
       "               FBL          FHD          TML          Sex  \n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  \n",
       "mean    422.533213    43.476951   352.984337     0.355366  \n",
       "std      31.435141     3.969791    28.390340     0.478781  \n",
       "min     329.750000    33.315000   271.000000     0.000000  \n",
       "25%     400.000000    40.638750   332.500000     0.000000  \n",
       "50%     423.000000    43.502500   353.125000     0.000000  \n",
       "75%     444.500000    46.286250   372.500000     1.000000  \n",
       "max     530.500000    57.625000   444.500000     1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_imputed_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data set using the iterative imputer\n",
    "\n",
    "iter_imputer = IterativeImputer(max_iter = 1000, missing_values=0.0)\n",
    "\n",
    "sex_column = model_data_goldman['Sex']\n",
    "\n",
    "temporary_data_set = model_data_goldman.fillna(0.).drop([\"Sex\"],axis=1)\n",
    "\n",
    "cols = temporary_data_set.columns\n",
    "\n",
    "temporary_data_set = iter_imputer.fit_transform(temporary_data_set)\n",
    "\n",
    "temporary_data_set = pd.DataFrame(data=temporary_data_set, columns=cols)\n",
    "\n",
    "iter_imputed_data_goldman = pd.concat([temporary_data_set,sex_column],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>HML</th>\n",
       "      <th>HHD</th>\n",
       "      <th>RML</th>\n",
       "      <th>FML</th>\n",
       "      <th>FBL</th>\n",
       "      <th>FHD</th>\n",
       "      <th>TML</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>262.198501</td>\n",
       "      <td>305.510815</td>\n",
       "      <td>42.852649</td>\n",
       "      <td>233.797950</td>\n",
       "      <td>426.440201</td>\n",
       "      <td>422.562111</td>\n",
       "      <td>43.479954</td>\n",
       "      <td>352.978508</td>\n",
       "      <td>0.355366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.082733</td>\n",
       "      <td>22.862662</td>\n",
       "      <td>4.119429</td>\n",
       "      <td>18.879454</td>\n",
       "      <td>31.496783</td>\n",
       "      <td>31.449895</td>\n",
       "      <td>3.974315</td>\n",
       "      <td>28.378283</td>\n",
       "      <td>0.478781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>179.500000</td>\n",
       "      <td>343.750000</td>\n",
       "      <td>329.750000</td>\n",
       "      <td>33.315000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>251.000000</td>\n",
       "      <td>289.500000</td>\n",
       "      <td>39.807500</td>\n",
       "      <td>220.250000</td>\n",
       "      <td>404.187500</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>40.635000</td>\n",
       "      <td>332.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>263.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>42.920000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>426.750000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>43.495000</td>\n",
       "      <td>353.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>321.062500</td>\n",
       "      <td>45.772500</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>46.297500</td>\n",
       "      <td>372.487319</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>55.120000</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>531.750000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>57.625000</td>\n",
       "      <td>444.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BIB          HML          HHD          RML          FML  \\\n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  1528.000000   \n",
       "mean    262.198501   305.510815    42.852649   233.797950   426.440201   \n",
       "std      18.082733    22.862662     4.119429    18.879454    31.496783   \n",
       "min     184.000000   234.000000    29.580000   179.500000   343.750000   \n",
       "25%     251.000000   289.500000    39.807500   220.250000   404.187500   \n",
       "50%     263.000000   305.000000    42.920000   234.000000   426.750000   \n",
       "75%     274.000000   321.062500    45.772500   247.500000   448.000000   \n",
       "max     324.000000   379.500000    55.120000   290.250000   531.750000   \n",
       "\n",
       "               FBL          FHD          TML          Sex  \n",
       "count  1528.000000  1528.000000  1528.000000  1528.000000  \n",
       "mean    422.562111    43.479954   352.978508     0.355366  \n",
       "std      31.449895     3.974315    28.378283     0.478781  \n",
       "min     329.750000    33.315000   271.000000     0.000000  \n",
       "25%     400.000000    40.635000   332.500000     0.000000  \n",
       "50%     423.000000    43.495000   353.250000     0.000000  \n",
       "75%     444.500000    46.297500   372.487319     1.000000  \n",
       "max     530.500000    57.625000   444.500000     1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_imputed_data_goldman.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data\n",
    "\n",
    "no_zeroes_model_data_goldman = no_zeroes_model_data_goldman.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "knn_imputed_data_goldman = knn_imputed_data_goldman.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "iter_imputed_data_goldman = iter_imputed_data_goldman.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = [\n",
    "    no_zeroes_model_data_goldman, \n",
    "    knn_imputed_data_goldman, \n",
    "    iter_imputed_data_goldman\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_names = [\n",
    "    \"Logistic Regression\", \n",
    "    \"Decision Tree Classifier\", \n",
    "    \"Support Vector Machines\", \n",
    "    \"Gaussian Process Classifier\", \n",
    "    \"Gradient Boosting Classifier\", \n",
    "    \"Random Forest Classifier\",\n",
    "    \"Ada Boost Classifier\", \n",
    "    \"Extra Trees Classifier\", \n",
    "    \"Gaussian Naive Bayes\", \n",
    "    \"KNNeighbors Classifier\",\n",
    "    \"Linear Discriminant Analysis\", \n",
    "    \"Quadratic Discriminant Analysis\", \n",
    "    \"XGBClassifier\", \n",
    "    \"Light Gradient Boosting Classifier\"\n",
    "]\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    GradientBoostingClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    XGBClassifier(),\n",
    "    lgb.LGBMClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "dataset_scores_list = []\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    scores = []\n",
    "    \n",
    "    X = dataset.drop('Sex', axis = 1).values\n",
    "    y = dataset['Sex']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size=0.3, stratify=y)\n",
    "    \n",
    "    for name, clf in zip(classifier_names, classifiers):\n",
    "        run_score = []\n",
    "        \n",
    "        for i in range(20):\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)*100\n",
    "            run_score.append(score)\n",
    "            \n",
    "            avg_score = np.mean(run_score)\n",
    "                      \n",
    "        #print(run_score)    \n",
    "        scores.append(avg_score)\n",
    "                  \n",
    "    dataset_scores_list.append(scores)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[78.85572139303483,\n",
       "  81.75373134328359,\n",
       "  76.61691542288557,\n",
       "  85.82089552238807,\n",
       "  86.54228855721392,\n",
       "  85.39800995024876,\n",
       "  86.31840796019901,\n",
       "  85.31094527363183,\n",
       "  79.10447761194027,\n",
       "  79.85074626865669,\n",
       "  86.56716417910448,\n",
       "  86.06965174129351,\n",
       "  86.31840796019901,\n",
       "  86.06965174129351],\n",
       " [81.04575163398695,\n",
       "  83.72549019607844,\n",
       "  77.7777777777778,\n",
       "  79.73856209150328,\n",
       "  87.59259259259258,\n",
       "  87.48366013071896,\n",
       "  87.14596949891069,\n",
       "  86.86274509803921,\n",
       "  81.69934640522874,\n",
       "  81.2636165577342,\n",
       "  88.6710239651416,\n",
       "  87.36383442265793,\n",
       "  88.23529411764706,\n",
       "  88.6710239651416],\n",
       " [84.96732026143789,\n",
       "  80.91503267973857,\n",
       "  80.82788671023965,\n",
       "  77.7777777777778,\n",
       "  90.55555555555556,\n",
       "  88.84531590413943,\n",
       "  89.54248366013073,\n",
       "  88.57298474945534,\n",
       "  82.35294117647057,\n",
       "  81.04575163398695,\n",
       "  90.6318082788671,\n",
       "  90.6318082788671,\n",
       "  89.54248366013073,\n",
       "  89.76034858387797]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(index=classifier_names)\n",
    "# results['name'] = names\n",
    "results['goldman_1'] = dataset_scores_list[0]\n",
    "results['goldman_2'] = dataset_scores_list[1]\n",
    "results['goldman_3'] = dataset_scores_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goldman_1</th>\n",
       "      <th>goldman_2</th>\n",
       "      <th>goldman_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>78.855721</td>\n",
       "      <td>81.045752</td>\n",
       "      <td>84.967320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>81.753731</td>\n",
       "      <td>83.725490</td>\n",
       "      <td>80.915033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machines</th>\n",
       "      <td>76.616915</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>80.827887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Process Classifier</th>\n",
       "      <td>85.820896</td>\n",
       "      <td>79.738562</td>\n",
       "      <td>77.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting Classifier</th>\n",
       "      <td>86.542289</td>\n",
       "      <td>87.592593</td>\n",
       "      <td>90.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <td>85.398010</td>\n",
       "      <td>87.483660</td>\n",
       "      <td>88.845316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ada Boost Classifier</th>\n",
       "      <td>86.318408</td>\n",
       "      <td>87.145969</td>\n",
       "      <td>89.542484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees Classifier</th>\n",
       "      <td>85.310945</td>\n",
       "      <td>86.862745</td>\n",
       "      <td>88.572985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>79.104478</td>\n",
       "      <td>81.699346</td>\n",
       "      <td>82.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors Classifier</th>\n",
       "      <td>79.850746</td>\n",
       "      <td>81.263617</td>\n",
       "      <td>81.045752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis</th>\n",
       "      <td>86.567164</td>\n",
       "      <td>88.671024</td>\n",
       "      <td>90.631808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quadratic Discriminant Analysis</th>\n",
       "      <td>86.069652</td>\n",
       "      <td>87.363834</td>\n",
       "      <td>90.631808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>86.318408</td>\n",
       "      <td>88.235294</td>\n",
       "      <td>89.542484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Light Gradient Boosting Classifier</th>\n",
       "      <td>86.069652</td>\n",
       "      <td>88.671024</td>\n",
       "      <td>89.760349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    goldman_1  goldman_2  goldman_3\n",
       "Logistic Regression                 78.855721  81.045752  84.967320\n",
       "Decision Tree Classifier            81.753731  83.725490  80.915033\n",
       "Support Vector Machines             76.616915  77.777778  80.827887\n",
       "Gaussian Process Classifier         85.820896  79.738562  77.777778\n",
       "Gradient Boosting Classifier        86.542289  87.592593  90.555556\n",
       "Random Forest Classifier            85.398010  87.483660  88.845316\n",
       "Ada Boost Classifier                86.318408  87.145969  89.542484\n",
       "Extra Trees Classifier              85.310945  86.862745  88.572985\n",
       "Gaussian Naive Bayes                79.104478  81.699346  82.352941\n",
       "KNNeighbors Classifier              79.850746  81.263617  81.045752\n",
       "Linear Discriminant Analysis        86.567164  88.671024  90.631808\n",
       "Quadratic Discriminant Analysis     86.069652  87.363834  90.631808\n",
       "XGBClassifier                       86.318408  88.235294  89.542484\n",
       "Light Gradient Boosting Classifier  86.069652  88.671024  89.760349"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iter_imputed_data_goldman.drop('Sex', axis = 1).values\n",
    "y = iter_imputed_data_goldman['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8322440087145969"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression model\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=10, estimator=LogisticRegression(),\n",
       "              search_spaces={'C': array([0.010000, 0.012664, 0.016037, 0.020309, 0.025719, 0.032570,\n",
       "       0.041246, 0.052233, 0.066147, 0.083768, 0.106082, 0.134340,\n",
       "       0.170125, 0.215443, 0.272833, 0.345511, 0.437548, 0.554102,\n",
       "       0.701704, 0.888624, 1.125336, 1.425103, 1.804722, 2.285464,\n",
       "       2.894266, 3.665241, 4.641589, 5.878016, 7.443803, 9.426685,\n",
       "       11.937766, 15.117751, 19.144820, 24.244620, 30.702906, 38.881552,\n",
       "       49.238826, 62.355073, 78.965229, 100.000000]),\n",
       "                             'max_iter': [1000, 1500, 2000],\n",
       "                             'random_state': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
       "                                              11]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the logistic regression model\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'C': np.logspace(-2,2,40),\n",
    "    'max_iter': [1000, 1500, 2000],\n",
    "    'random_state': [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "}\n",
    "\n",
    "clf  = BayesSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('C', 0.03257020655659783),\n",
       "             ('max_iter', 2000),\n",
       "             ('random_state', 5)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     89.98\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7821350762527233"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support vector machines\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=SVC(),\n",
       "                   param_distributions={'C': array([0.010000, 0.027826, 0.077426, 0.215443, 0.599484, 1.668101,\n",
       "       4.641589, 12.915497, 35.938137, 100.000000]),\n",
       "                                        'kernel': ['rbf', 'linear']})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Support Vevtor Machine model\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "parameters = {\n",
    "    'C': np.logspace(-2,2,10),\n",
    "    'kernel': ['rbf','linear']\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'linear', 'C': 0.5994842503189409}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     91.29\n"
     ]
    }
   ],
   "source": [
    "model = SVC(**clf.best_params_, probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7973856209150327"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kNN classifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=KNeighborsClassifier(),\n",
       "                   param_distributions={'leaf_size': [1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                                                      9],\n",
       "                                        'metric': ['euclidean', 'manhattan'],\n",
       "                                        'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                                                        9, 10, 11, 12, 13, 14,\n",
       "                                                        15, 16, 17, 18, 19,\n",
       "                                                        20],\n",
       "                                        'weights': ['uniform', 'distance']})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the kNN classifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_neighbors': list(range(1,21)),\n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'metric' : ['euclidean', 'manhattan'],\n",
    "    'leaf_size': list(range(1,10))\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': 'distance',\n",
       " 'n_neighbors': 6,\n",
       " 'metric': 'manhattan',\n",
       " 'leaf_size': 2}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     83.01\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8387799564270153"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=GaussianNB(),\n",
       "             param_grid={'var_smoothing': array([1.000000, 0.811131, 0.657933, 0.533670, 0.432876, 0.351119,\n",
       "       0.284804, 0.231013, 0.187382, 0.151991, 0.123285, 0.100000,\n",
       "       0.081113, 0.065793, 0.053367, 0.043288, 0.035112, 0.028480,\n",
       "       0.023101, 0.018738, 0.015199, 0.012328, 0.010000, 0.008111,\n",
       "       0.006579, 0.005337, 0.004329, 0.003511, 0.002848, 0.002310,\n",
       "       0.0...\n",
       "       0.000004, 0.000003, 0.000002, 0.000002, 0.000002, 0.000001,\n",
       "       0.000001, 0.000001, 0.000001, 0.000001, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,\n",
       "       0.000000, 0.000000, 0.000000, 0.000000])})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Gaussian Naive Bayes classifier\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "parameters = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "             }\n",
    "\n",
    "clf  = GridSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'var_smoothing': 0.001}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     83.88\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9084967320261438"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=10, estimator=LinearDiscriminantAnalysis(),\n",
       "              search_spaces={'solver': ['svd', 'lsqr', 'eigen']})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Linear Discriminant Analysis classifier\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "parameters = {\n",
    "    'solver' : ['svd', 'lsqr', 'eigen']\n",
    "}\n",
    "\n",
    "clf  = BayesSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('solver', 'svd')])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     90.85\n"
     ]
    }
   ],
   "source": [
    "model = LinearDiscriminantAnalysis(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(model, open(\"lda_model.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8736383442265795"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quadratic Discriminant Analysis\n",
    "\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=QuadraticDiscriminantAnalysis(),\n",
       "             param_grid={'reg_param': [0.0, 0.1, 0.2, 0.3, 0.4]})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Quadratic Discriminant Analysis classifier\n",
    "\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "parameters = {\n",
    "    'reg_param' : [0., 0.1, 0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "clf  = GridSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_param': 0.1}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     88.45\n"
     ]
    }
   ],
   "source": [
    "model = QuadraticDiscriminantAnalysis(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8082788671023965"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'criterion': ['entropy', 'gini'],\n",
       "                         'max_depth': [1, 2, 3, 4, 5, 6, 7, 15, 20, 30, 40, 120,\n",
       "                                       150]})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Decision Tree Classifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'criterion':['entropy','gini'],\n",
    "    'max_depth':[1,2,3,4,5,6,7,15,20,30,40,120,150]\n",
    "}\n",
    "\n",
    "clf  = GridSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 5}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     85.40\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(\"{:10.2f}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8976034858387799"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=RandomForestClassifier(), n_iter=20,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Random Forest Classifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, param_distributions=random_grid, n_iter = 20, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1800,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 50,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.23529411764706\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)*100\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8976034858387799"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost Classifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=XGBClassifier(),\n",
       "                   param_distributions={'colsample_bytree': [0.6, 0.8, 1.0],\n",
       "                                        'gamma': [0.1, 0.5, 1, 1.5, 2, 5],\n",
       "                                        'max_depth': [3, 4, 5, 5, 6, 7, 8],\n",
       "                                        'min_child_weight': [1, 2, 3, 4, 5, 6,\n",
       "                                                             25],\n",
       "                                        'subsample': [0.6, 0.8, 1.0]})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the XGBoost Classifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'min_child_weight': [1, 2, 3, 4, 5, 6, 25],\n",
    "    'gamma': [0.1, 0.5, 1, 1.5, 2, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 4, 5, 5, 6, 7, 8]\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.8,\n",
       " 'min_child_weight': 1,\n",
       " 'max_depth': 5,\n",
       " 'gamma': 1,\n",
       " 'colsample_bytree': 0.8}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8954248366013072\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(model, open(\"pima.pickle.dat\", \"wb\"))\n",
    "\n",
    "# iter_imputed_data_goldman.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7690631808278867"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Process Classifier\n",
    "\n",
    "model = GaussianProcessClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 122-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 129-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 129-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 129-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 129-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 129-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 129-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 129-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 129-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 664, in fit\n",
      "    self.base_estimator_.fit(X, y)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 212, in fit\n",
      "    optima = [self._constrained_optimization(obj_func,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 445, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\", line 623, in minimize\n",
      "    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 267, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 204, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 364, in log_marginal_likelihood\n",
      "    self._posterior_mode(K, return_temporaries=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py\", line 417, in _posterior_mode\n",
      "    L = cholesky(B, lower=True)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: 129-th leading minor of the array is not positive definite\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py:448: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:402: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.789272 nan 0.798426 0.877550 0.644634]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=GaussianProcessClassifier(),\n",
       "                   param_distributions={'kernel': [1**2 * RBF(length_scale=1),\n",
       "                                                   1**2 * DotProduct(sigma_0=1),\n",
       "                                                   1**2 * Matern(length_scale=1, nu=1.5),\n",
       "                                                   1**2 * RationalQuadratic(alpha=1, length_scale=1),\n",
       "                                                   1**2 * WhiteKernel(noise_level=1)]})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the GaussianProcessClassifier\n",
    "\n",
    "model = GaussianProcessClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'kernel' : [1*RBF(), 1*DotProduct(), 1*Matern(),  1*RationalQuadratic(), 1*WhiteKernel()]\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 1**2 * RationalQuadratic(alpha=1, length_scale=1)}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "model = GaussianProcessClassifier(**clf.best_params_, max_iter_predict = 1000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8845315904139434"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=10, estimator=GradientBoostingClassifier(),\n",
       "              search_spaces={'learning_rate': [0.01, 0.1, 1, 10, 100],\n",
       "                             'max_depth': [1, 3, 5, 7, 9],\n",
       "                             'n_estimators': [5, 50, 250, 500]})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Gradient Boosting Classifier\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\":[5,50,250,500],\n",
    "    \"max_depth\":[1,3,5,7,9],\n",
    "    \"learning_rate\":[0.01,0.1,1,10,100]\n",
    "}\n",
    "\n",
    "clf  = BayesSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('learning_rate', 0.01), ('max_depth', 5), ('n_estimators', 500)])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8867102396514162\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8801742919389978"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ada Boost Classifier\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=AdaBoostClassifier(),\n",
       "                   param_distributions={'learning_rate': [0.01, 0.1, 1, 10,\n",
       "                                                          100],\n",
       "                                        'n_estimators': [5, 50, 250, 500]})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the Gradient Boosting Classifier\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\":[5,50,250,500],\n",
    "    \"learning_rate\":[0.01,0.1,1,10,100]\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 250, 'learning_rate': 0.1}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8932461873638344\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(model, open(\"ada_boost_model.dat\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8932461873638344"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extra trees classifier\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 171, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 903, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    raise ValueError(\"min_samples_split must be an integer \"\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/cconsta1/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.841581 0.849458 0.861859 0.850103 0.855323 0.848800 0.844892 nan\n",
      " 0.858591 0.861193]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=ExtraTreesClassifier(),\n",
       "                   param_distributions={'min_samples_leaf': [1, 2, 3, 4, 5, 6,\n",
       "                                                             7, 8, 9, 10, 11,\n",
       "                                                             12, 13, 14, 15, 16,\n",
       "                                                             17, 18, 19],\n",
       "                                        'min_samples_split': [1, 2, 3, 4, 5, 6,\n",
       "                                                              7, 8, 9, 10, 11,\n",
       "                                                              12, 13, 14, 15,\n",
       "                                                              16, 17, 18, 19],\n",
       "                                        'n_estimators': [50, 75, 100, 125]})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the SGDClassifier\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators': list(range(50,126,25)),\n",
    "    'min_samples_leaf': list(range(1,20,1)),\n",
    "    'min_samples_split': list(range(1,20,1))\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 125, 'min_samples_split': 12, 'min_samples_leaf': 2}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823529411764706\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8649237472766884"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Light boosting regressor\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=LGBMClassifier(),\n",
       "                   param_distributions={'min_child_samples': [20, 30, 50, 100],\n",
       "                                        'min_child_weight': [1e-05, 0.001, 0.01,\n",
       "                                                             0.1, 1],\n",
       "                                        'num_leaves': [5, 10, 20, 31, 50, 100],\n",
       "                                        'reg_alpha': [0, 0.1, 1],\n",
       "                                        'reg_lambda': [0, 0.1, 1, 5, 10]})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing the SGDClassifier\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'num_leaves': [5, 10, 20, 31, 50, 100], \n",
    "    'min_child_samples': [20, 30, 50 , 100], \n",
    "    'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1],\n",
    "    'reg_alpha': [0, 1e-1, 1],\n",
    "    'reg_lambda': [0, 1e-1, 1, 5, 10]\n",
    "}\n",
    "\n",
    "clf  = RandomizedSearchCV(model, parameters, cv=10, return_train_score=False)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_lambda': 1,\n",
       " 'reg_alpha': 0.1,\n",
       " 'num_leaves': 5,\n",
       " 'min_child_weight': 1e-05,\n",
       " 'min_child_samples': 30}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.906318082788671\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(model, open(\"lgb_model.dat\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
